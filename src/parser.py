"""
Parser for the pytoc Python transpiler. 
"""

from copy import deepcopy

from src.utils.error import *
from src.utils.header import STDLIB_TO_MODULE_PATH_DICT
from src.utils.operators import BINOP_OPERATOR_PRECEDENCE_DICT, CONDITION_OPERATOR_PRECEDENCE_DICT, SLICE_OPERATOR_PRECEDENCE_DICT, ASSIGNMENT_OPERATOR_PRECEDENCE_DICT, EXPR_MAP, OPERATORS, UNOP_OPERATOR_DICT
from src.utils.tokens import Token
from src.utils.list_util_funcs import get_sublists, get_combinations
from src.utils.allowed_type_constants import *
from src.utils.built_in_funcs import BUILT_IN_FUNC_NAMES, VAR_ARG_BUILT_IN_FUNCS
from src.identifier_manager import IdentifierManager, IdentifierContainer
from src.nodes import *
from src.lexer import get_token_ident

class Parser():
    def __init__(self, tokens: list[Token], file_n: str, flags: dict[str: str | None]) -> None:
        """
        Attributes:

        error: Main location for all errors, all errors are 
        stored in this variable

        tokens: Tokens generated by the parser

        file_n: Target file name

        func_identifier_dict: Dictionary containing all function identifiers
        and their respective FuncDefNode

        var_identifier_dict: Same as func_identifier_dict but for 
        variables and only for the global scope

        indentation: First indentation amount used by the user, 
        error is thrown if other amount is used

        cur_block_indentation: The indentation amount of the
        block currently processed by parse_block()

        ast: Main instance of AST(), later returned by make_ast()
        """
        self.error = None
        self.warning = None
        self.tokens = tokens
        self.file_n = file_n
        self.flags = flags
        self.indentation = None
        self.cur_block_indentation = None
        self.special_globals = []
        self.ast = AST()
        self.identifier_manager = IdentifierManager(self.ast)
        self.ast_init()

    def ast_init(self) -> None:
        """
        Initializes the argc and argv commandline arguments
        """
        self.ast.append_node(AssignNode("argc", None))
        self.ast.base_node.children[0].type = ("int",)
        self.ast.base_node.children[0].id = -1
        self.ast.append_node(AssignNode("argv", None))
        self.ast.base_node.children[1].type = ("list",)
        self.ast.base_node.children[1].id = -2
        self.ast.next_free_id = 1
        self.identifier_manager.set_identifier("argc", self.ast.base_node.children[0])
        self.identifier_manager.set_identifier("argv", self.ast.base_node.children[1])

    def make_ast(self) -> AST:
        """
        Generates an AST from tokens provided by the Lexer Class

        Returns -1 on error, otherwise the generated ast and the identifiers of the global module scope 
        """
        if self.parse_block(self.tokens) == -1:
            return None
        self.ast.base_node.children = self.ast.base_node.children[2:] # Removes temporary nodes for argc and argv from the ast
        
        return self.ast, self.identifier_manager, self.special_globals
    
    #################################General Parsing######################################
    
    def parse_block(self, tokens: list[Token]) -> None:
        """
        Parses a block of tokens, a block being lines on the same or a higher indentation level than the first token passed

        Returns -1 on error, otherwise 0
        """
        line_tokens = self.get_line_tokens(tokens)
        i = 0
        while i < len(line_tokens):
            if line_tokens[i][0].token_t == "TT_eof": break
            i = self.parse_line(i, line_tokens[i], line_tokens[i+1:])
            if i == -1:
                return i
        self.cur_block_indentation = None
        return 0
    
    def parse_line(self, i: int, line: list[Token], rem_line_tokens: list[list[Token]]) -> int:
        """
        Takes a line represented by tokens, the line number of the line and the remaining tokens of the current block and tries to match it to the syntax of the different statements.

        Throws an error and returns -1 when given line doesnt match any statement
        """
        if len(line) == 1 and line[0].token_t == "TT_eof":
            return i+1
        
        cur_line_indentation = self.get_indentation(line[0])
        if cur_line_indentation == -1: return -1
        line = line[1:] if cur_line_indentation != 0 else line

        if self.is_assign(line):
            if self.parse_assign(line) == -1: return -1
            return i+1
        elif line[0].token_t == "TT_continue":
            if self.parse_continue_statement(line) == -1: return -1
            return i+1
        elif line[0].token_t == "TT_pass":
            if self.parse_pass_statement(line) == -1: return -1
            return i+1
        elif line[0].token_t == "TT_break":
            if self.parse_break_statement(line) == -1: return -1
            return i+1
        elif line[0].token_t == "TT_import":
            if self.parse_import_statement(line) == -1: return -1
            return i+1
        elif line[0].token_t == "TT_identifier" and line[1].token_t == "TT_lparen" and line[len(line)-2].token_t == "TT_rparen":
            if self.parse_func_call(line) == -1: return -1
            return i+1
        elif line[0].token_t in ("TT_if", "TT_elif", "TT_else"):
            line_count = self.parse_conditional_statement(line, rem_line_tokens, cur_line_indentation)
            if line_count == -1: return -1
            return i+line_count+1
        elif line[0].token_t == "TT_while":
            line_count = self.parse_while_loop(line, rem_line_tokens, cur_line_indentation)
            if line_count == -1: return -1
            return i+line_count+1
        elif line[0].token_t == "TT_for":
            line_count = self.parse_for_loop(line, rem_line_tokens, cur_line_indentation)
            if line_count == -1: return -1
            return i+line_count+1
        elif line[0].token_t == "TT_def":
            line_count = self.parse_func_def(line, rem_line_tokens, cur_line_indentation)
            if line_count == -1: return -1
            return i+line_count+1
        elif line[0].token_t == "TT_ret":
            if self.parse_return_statement(line) == -1: return -1
            return i+1
        
        self.error = SyntaxError("Invalid Syntax", line[0].ln, self.file_n)
        return -1
    
    ###########################Control Flow and Statement Parsing################################
    
    def parse_break_statement(self, line: list[Token]) -> int:
        """
        Parses a break statement represented in tokens to its corresponding AST Node while making sure that the statement is located in a loop

        Returns -1 on error, otherwise 0
        """
        cur_ln_num = line[0].ln
        line = line[:-1]
        if len(line) > 1:
            self.error = SyntaxError("Invalid Syntax", cur_ln_num, self.file_n)
            return -1
        
        new_node_id = self.ast.append_node(BreakNode())
        self.ast.traverse_node_by_id(new_node_id)

        parent_node = self.ast.get_parent_node((ForLoopNode, WhileLoopNode))
        if not parent_node:
            self.error = SyntaxError("Continue Statement not properly inside loop", cur_ln_num, self.file_n)
            return -1

        self.ast.detraverse_node()

        return 0
    
    def parse_pass_statement(self, line: list[Token]) -> int:
        """
        Parses a pass statement into its corresponding AST Node

        Returns -1 on error, otherwise 0
        """
        cur_ln_num = line[0].ln
        line = line[:-1]
        if len(line) > 1:
            self.error = SyntaxError("Invalid Syntax", cur_ln_num, self.file_n)
            return -1
        
        new_node_id = self.ast.append_node(PassNode())

        return 0
    
    def parse_continue_statement(self, line: list[Token]) -> int:
        """
        Parses a continue statement into its corresponding AST Node while making sure that is is located inside a loop

        Returns -1 on error, otherwise 0
        """
        cur_ln_num = line[0].ln
        line = line[:-1]
        if len(line) > 1:
            self.error = SyntaxError("Invalid Syntax", cur_ln_num, self.file_n)
            return -1
        
        new_node_id = self.ast.append_node(ContinueNode())
        self.ast.traverse_node_by_id(new_node_id)

        parent_node = self.ast.get_parent_node(ForLoopNode)
        if not parent_node:
            self.error = SyntaxError("Continue Statement not properly inside loop", cur_ln_num, self.file_n)
            return -1

        self.ast.detraverse_node()

        return 0
    
    def parse_import_statement(self, line: list[Token]) -> int:
        """
        Parses an import statement. Import statements are not realized through modules but through c-like includes.

        Returns -1 on error, otherwise 0
        """
        cur_ln_num = line[0].ln
        line = line[1:-1]
        module_path = self.get_module_path(line, cur_ln_num) #Obtains the real path to the included module from the source directory
        if module_path == -1: return -1

        if module_path in STDLIB_TO_MODULE_PATH_DICT.keys():
            module_path = STDLIB_TO_MODULE_PATH_DICT[module_path] #Adjusts stdlib modules like "os" to their real path from the source

        self.ast.append_node(ImportNode(module_path))

        from src.compiler import Compiler

        flags = deepcopy(self.flags) # Copies the flags given
        flags["--import"] = None # Adds the "--import" flag, prompting the compiler to return the ast and identifiers after the parsing stage without generating code

        import_compiler = Compiler(module_path, flags)
        import_ast, import_ident_man, special_globals = import_compiler.compile()
        
        self.special_globals += special_globals #Adds the special globals from the imported module to the current module

        args = (self.ast.base_node.children.pop(0), self.ast.base_node.children.pop(0)) #Saves the commandline arguments of the current module
        self.ast.base_node.children = import_ast.base_node.children + self.ast.base_node.children #Merges the ASTs of the current and the imported module
        for i in range(len(args)):
            self.ast.base_node.children.insert(i, args[i]) #Inserts the commandline arguments at the start again

        self.identifier_manager.merge_identifiers(import_ident_man) #Merges the global scope identifiers from the current and the imported module    
        self.ast.readjust_ids()

        return 0

    def get_module_path(self, line: list[Token], ln_num: int) -> str:
        """
        Extracts the path to the module imported in an import statement
        """
        req_identifier = True
        path_str = ""
        for token in line:
            if req_identifier:
                if token.token_t != "TT_identifier":
                    self.error = SyntaxError("Invalid Syntax", ln_num, self.file_n)
                    return -1
                path_str += token.token_v
            else:
                if token.token_t != "TT_dot":
                    self.error = SyntaxError("Invalid Syntax", ln_num, self.file_n)
                    return -1
                path_str += "/"
            req_identifier = not req_identifier
        
        if req_identifier:
            self.error = SyntaxError("Invalid Syntax", ln_num, self.file_n)
            return -1

        return path_str + ".py"


    def parse_return_statement(self, line: list[Token]) -> int:
        """
        Parses a return statement to its corresponding AST Node

        Returns -1 on error, otherwise 0
        """
        cur_ln_num = line[0].ln
        line = line[1:]
        #Looks for FuncDefNode as parent node because then it would be in a function
        parent_node = self.ast.get_parent_node(FuncDefNode)
        if parent_node == -1:
            self.error = SyntaxError("Return Statement outside function!", cur_ln_num, self.file_n)
            return -1
        new_node_id = self.ast.append_node(ReturnNode())
        self.ast.traverse_node_by_id(new_node_id)
        return_type = self.parse_expression(line, "return_value", cur_ln_num, expr_4=False)
        if return_type == -1: return -1
        self.ast.cur_node.type = return_type
        parent_node.return_type = ()
        parent_node.return_nodes += [self.ast.cur_node]
        self.ast.detraverse_node()
        return 0
    
    def parse_func_def(self, line: list[Token], rem_line_tokens: list[list[Token]], cur_line_indentation: int) -> int:
        """
        Takes a list of tokens, the remaining tokens in the current block and the indentation of the current line and parses them to their AST Node
        It does not parse the function body except for when the --import flag is given, as that is parsed only after the first function call is parsed

        Returns -1 on error, otherwise the amount of lines in the function body
        """
        cur_ln_num = line[0].ln
        #if second last character not a colon: error
        if line[len(line)-2].token_t != "TT_colon":
            self.error = SyntaxError("Expected colon", cur_ln_num, self.file_n)
            return -1
        line = line[1:len(line)-2]
        if line[0].token_t != "TT_identifier":
            self.error = SyntaxError("Expected identifier", cur_ln_num, self.file_n)
            return -1
        func_identifier, line = line[0].token_v, line[1:]
        func_identifier = self.identifier_manager.generate_relative_identifier(func_identifier)
        if line[0].token_t != "TT_lparen" or line[len(line)-1].token_t != "TT_rparen":
            self.error = SyntaxError("Expected parenthesis", cur_ln_num, self.file_n)
            return -1
        line = line[1:len(line)-1]
        args = self.parse_func_def_args(line, cur_ln_num)
        if args == -1: return args
        if self.ast.get_parent_node(FuncDefNode, WhileLoopNode, ForLoopNode, IfNode, ElifNode, ElseNode) != -1:
            self.error = SyntaxError("Can only define a function in the global scope or in a class", cur_ln_num, self.file_n)
            return -1
        new_node_id = self.ast.append_node(FuncDefNode(func_identifier, args))
        self.ast.traverse_node_by_id(new_node_id)
        self.identifier_manager.set_identifier(func_identifier, self.ast.cur_node, force_global=True, is_func=True)
        self.ast.cur_node.identifier_container = IdentifierContainer()
        self.ast.cur_node.arg_names = args
        self.ast.cur_node.indentation = cur_line_indentation
        children = self.get_children(rem_line_tokens, cur_line_indentation)
        if children == -1: return -1
        children_count, self.ast.cur_node.unparsed_children = children
        if "--import" in self.flags.keys():
            self.parse_no_discard_func_def()
        self.ast.detraverse_node()
        return children_count
    
    def parse_no_discard_func_def(self) -> None:
        """
        Parses the body of the function definition in case the --import flag is given without infering types as the argument types are not known
        """
        arg_types = [() for i in range(len(self.ast.cur_node.arg_names))]
        self.parse_func_def_children(arg_types, self.ast.cur_node, self.ast.cur_node)
        return None
    
    def parse_func_def_args(self, tokens: list[Token], cur_ln_num: int) -> list[str] | int:
        """
        Takes the tokens representing the function arguments and parses them into a list of expressions represented by their tokens
        Returns -1 on error, otherwise a list of the argument names
        """
        if not tokens: return []
        args = []
        cur_token = None
        for token in tokens:
            cur_token = token
            if token.token_t == "TT_identifier":
                args.append(token.token_v)
                continue
            if token.token_t == "TT_comma":
                continue
            self.error = SyntaxError("Invalid Syntax", cur_ln_num, self.file_n)
            return -1
        if cur_token.token_t == "TT_comma":
            self.error = SyntaxError("Expected argument", cur_ln_num, self.file_n)
            return -1
        return args

    def parse_for_loop(self, line: list[Token], rem_line_tokens: list[list[Token]], cur_line_indentation: int) -> int:
        """
        Parses a for loop and its body represented by a list of tokens into its AST Node

        Returns -1 on error, otherwise the amount of lines in the body
        """
        cur_ln_num = line[0].ln
        if line[len(line)-2].token_t != "TT_colon":
            self.error = SyntaxError("Expected colon", cur_ln_num, self.file_n)
            return -1
        line = line[1:len(line)-2]
        if line[0].token_t != "TT_identifier":
            self.error = SyntaxError("Expected identifier", cur_ln_num, self.file_n)
            return -1
        if line[1].token_t != "TT_in":
            self.error = SyntaxError("Invalid Syntax", line[0].ln, self.file_n)
            return -1
        iter_var_name = line[0].token_v
        new_node_id = self.ast.append_node(ForLoopNode(self.identifier_manager.generate_relative_identifier(iter_var_name), line[0].ln))
        line = line[2:]
        self.ast.traverse_node_by_id(new_node_id)
        expr_type = self.parse_expression(line, "iter", cur_ln_num, expr_2=False, expr_4=False, expr_6=False, expr_7=False, expr_8=False)
        if expr_type == -1: return -1
        if not self.is_valid_type(expr_type, ("str", "list")): 
            self.error = TypeError(f"Object of type '{expr_type} is not iterable'", cur_ln_num, self.file_n)
            return -1
        iter_var_node = AssignNode(self.identifier_manager.get_relative_identifier(iter_var_name), None)
        self.ast.append_node(iter_var_node)
        if isinstance(self.ast.cur_node.iter, FuncCallNode) and self.ast.cur_node.iter.name in BUILT_IN_FUNC_NAMES:
            self.ast.cur_node.children[0].type = ()
        else:
            self.ast.cur_node.children[0].type = self.get_array_types() if self.ast.cur_node.iter.type != () else ()
        self.identifier_manager.set_identifier(iter_var_name, iter_var_node)
        child_count = self.parse_children(cur_line_indentation, rem_line_tokens)
        self.ast.detraverse_node()
        return child_count

    def parse_while_loop(self, line: list[Token], rem_line_tokens: list[list[Token]], cur_line_indentation: int) -> int:
        """
        Takes the token representation of a while loop and parses it and its body into their AST Nodes

        Returns -1 on error, otherwise the amount of lines inside the body
        """
        cur_ln_num = line[0].ln
        if line[len(line)-2].token_t != "TT_colon":
            self.error = SyntaxError("Expected colon", line[0].ln, self.file_n)
            return -1
        line = line[1:len(line)-2]
        new_node_id = self.ast.append_node(WhileLoopNode(line[0].ln))
        self.ast.traverse_node_by_id(new_node_id)
        expr_type = self.parse_expression(line, "condition",cur_ln_num,  expr_4=False)
        if expr_type == -1: return -1
        if not self.is_valid_type(expr_type, ("bool",)):
            self.error = TypeError("Invalid type for conditional expression", cur_ln_num, self.file_n)
            return -1
        child_count = self.parse_children(cur_line_indentation, rem_line_tokens)
        self.ast.detraverse_node()
        return child_count
        
    def parse_conditional_statement(self, line: list[Token], rem_line_tokens: list[list[Token]], cur_line_indentation: int) -> int:
        """
        Takes the token representation of a conditional statement and parses it and its body into their corresponding AST Nodes

        Returns -1 on error, otherwise the amount of lines in the body
        """
        cur_ln_num = line[0].ln
        if line[len(line)-2].token_t != "TT_colon":
            self.error = SyntaxError("Expected colon at end of conditional statement", cur_ln_num, self.file_n)
            return -1
        match line[0].token_t:
            case "TT_if": kw_node = IfNode()
            case "TT_elif": kw_node = ElifNode()
            case "TT_else": kw_node = ElseNode()
        new_node_id = self.ast.append_node(kw_node)
        self.ast.traverse_node_by_id(new_node_id)
        if line[0].token_t in ("TT_elif", "TT_else"):
            if self.get_prev_conditional_nodes() == -1: 
                self.error = SyntaxError("Expected conditional statement before 'else/elif'", line[0].ln, self.file_n)
                return -1
        line = line[1:len(line)-2]
        if not isinstance(self.ast.cur_node, ElseNode):
            line = self.merge_equ(line)
            if self.parse_expression(line, "condition", cur_ln_num, expr_4=False) == -1: return -1
        child_count = self.parse_children(cur_line_indentation, rem_line_tokens)
        self.ast.cur_node.line = cur_ln_num
        self.ast.detraverse_node()
        return child_count

    def parse_children(self, parent_line_indentation: int, child_line_tokens: list[list[Token]] = None, block_to_parse: list[list[Token]] = None) -> int:
        """
        Parses the body of a statement containing an indented block into their respective AST Nodes

        Returns -1 on error, otherwise the amount of lines in the block
        """
        if block_to_parse:
            children_count = len(block_to_parse)
        else: 
            result = self.get_children(child_line_tokens, parent_line_indentation)
            if result == -1: return -1
            children_count, block_to_parse = result
        block_to_parse = [token for line in block_to_parse for token in line]
        old_cur_block_indentation = self.cur_block_indentation
        self.cur_block_indentation = None
        if self.parse_block(block_to_parse) == -1: return -1
        self.cur_block_indentation = old_cur_block_indentation
        return children_count

    def get_prev_conditional_nodes(self) -> None:
        """
        Lists all conditional statement Nodes located directly before the current node in the current block and adds them to the current node

        Returns -1 on error, otherwise 0
        """
        cur_node = self.ast.cur_node
        prev_cond_nodes = []
        while True:
            if self.ast.prev_child_node() == -1: break
            if not isinstance(self.ast.cur_node, (IfNode, ElifNode)):
                break
            prev_cond_nodes.append(self.ast.cur_node)
        if prev_cond_nodes == []:
            return -1
        self.ast.cur_node = cur_node
        for node in prev_cond_nodes:
            self.ast.cur_node.prev_conditions.append(node)
        return 0

    def parse_func_call(self, line: list[Token], traversal_type: str="children", from_expr: bool = False) -> int:
        """
        Takes the token representation of a function call expression and parses it into an AST Node.
        Also parses the body of the function definition of the called function if it is the first call to that function and the --import flag has not been given

        Returns -1 on error, otherwise 0
        """
        cur_ln_num = line[0].ln
        name = line[0].token_v
        if not self.identifier_manager.identifier_exists(name):
            self.error = NameError(f"Call to undefined function {name}()", cur_ln_num, self.file_n) 
            return -1
        if not from_expr:
            line = line[2:len(line)-2]
        else:
            line = line[2:len(line)-1]
        arg_exprs = self.parse_func_call_args(line)
        if isinstance(arg_exprs, int):
           return self.handle_arg_parsing_error(arg_exprs, cur_ln_num)
        new_node_id = self.ast.append_node(FuncCallNode(self.identifier_manager.get_invalid_identifier(name), cur_ln_num), traversal_type)
        self.ast.traverse_node_by_id(new_node_id, traversal_type)
        new_node = self.ast.cur_node
        arg_types = []
        for arg_expr in arg_exprs:
            arg_type = self.parse_expression(arg_expr, "args", cur_ln_num, expr_4=False)
            if arg_type == -1: return -1
            arg_types.append(arg_type)
        if not arg_exprs and name == "input":
            self.ast.append_node(StringNode(""), "args")
        elif len(arg_exprs) == 1 and name == "strip":
            self.ast.append_node(StringNode(" "), "args")
        elif not arg_exprs and name == "print":
            self.ast.append_node(StringNode(""), "args")
        elif not arg_exprs and name == "exit":
            self.ast.append_node(NumberNode(0), "args")
        last_func_def_node = self.identifier_manager.get_identifier_node(name)
        var_args = self.var_args_checking(name, arg_types, cur_ln_num)
        if self.error: return -1
        if not var_args and len(arg_exprs) != len(last_func_def_node.arg_names):
            self.error = TypeError(f"{name}() takes {len(last_func_def_node.arg_names)} arguments but {len(arg_exprs)} were given!", cur_ln_num, self.file_n)
            return -1
        if last_func_def_node.func_call_nodes:
            last_func_def_node.func_call_nodes.append(new_node)
        else:
            last_func_def_node.func_call_nodes = [new_node]
        if name not in BUILT_IN_FUNC_NAMES and last_func_def_node.unparsed_children:
            if self.parse_func_def_children(arg_types, last_func_def_node, new_node) == -1: return -1
        self.ast.detraverse_node()
        return 0
    
    def handle_arg_parsing_error(self, error_code: int, cur_ln_num: int) -> int:
        """
        Throws different errors depending on the error code given as an argument

        Returns -1
        """
        match error_code:
            case -1: 
                    self.error = SyntaxError("'(' was never closed", cur_ln_num, self.file_n)
            case -2:
                    self.error = SyntaxError("'[' was never closed", cur_ln_num, self.file_n)
            case -3:
                    self.error = SyntaxError("Invalid Syntax", cur_ln_num, self.file_n)
        return -1

    def parse_func_def_children(self, arg_types: list[str], last_func_def_node: FuncDefNode, new_node: FuncCallNode) -> None:
        """
        Parses the body of a function definition, represented by tokens into their AST Node

        Returns -1 on error, otherwise 0
        """
        self.ast.cur_node = last_func_def_node
        self.ast.cur_node.arg_types = arg_types
        block_to_parse = self.ast.cur_node.unparsed_children
        self.ast.cur_node.unparsed_children = None
        for i in range(len(arg_types)):
            self.ast.append_node(AssignNode(self.ast.cur_node.arg_names[i], None))
            self.identifier_manager.set_identifier(self.ast.cur_node.arg_names[i], self.ast.cur_node.children[i])
            self.ast.cur_node.children[i].type = arg_types[i]
        if self.parse_children(self.ast.cur_node.indentation, block_to_parse=block_to_parse) == -1: return -1
        if not self.ast.cur_node.return_nodes:
            self.ast.append_node(ReturnNode(), "return_nodes")
            self.ast.traverse_node("return_nodes")
            self.ast.append_node(NoneNode(), "return_value")
            self.ast.detraverse_node()
            self.ast.cur_node.children.append(self.ast.cur_node.return_nodes[0])
        self.ast.cur_node = new_node
        return 0
    
    def parse_func_call_args(self, tokens: list[Token]) -> list[list[Token]]:
        """
        Parses the arguments to a function call represented by tokens into a list of expressions represented by tokens
        
        Returns an error code between -1 and -3 on error, otherwise a list of the expression passed as arguments
        """
        if not tokens: return []
        arg_exprs = []
        cur_arg_expr = []
        paren_depth = 0
        bracket_depth = 0
        for token in tokens:
            if token.token_t in ("TT_lparen", "TT_rparen", "TT_lbracket", "TT_rbracket"):
                match token.token_t:
                    case "TT_lparen":
                        paren_depth += 1
                    case "TT_rparen":
                        if paren_depth == 0: return -1
                        paren_depth -= 1
                    case "TT_lbracket":
                        bracket_depth += 1
                    case "TT_rbracket":
                        if bracket_depth == 0: return -1
                        bracket_depth -= 1
                cur_arg_expr.append(token)
                continue
            if token.token_t == "TT_dquote" or token.token_t == "TT_squote":
                continue
            elif token.token_t == "TT_comma" and not paren_depth and not bracket_depth:
                arg_exprs.append(cur_arg_expr)
                cur_arg_expr = []
            else:
                cur_arg_expr.append(token)
        if paren_depth:
            return -1
        if bracket_depth:
            return -2
        if cur_arg_expr == []:
            return -3
        arg_exprs.append(cur_arg_expr)
        return arg_exprs
    
    def parse_assign(self, line: list[Token]) -> int:
        """
        Parses an assign expression in token representation into its ASTNode

        Returns -1 on error, otherwise 0
        """
        cur_ln_num = line[0].ln
        line = line[:len(line)-1]
        name = line[0].token_v
        if name in ("argc", "argv"):
            self.warning = Warning(f"Overwriting built-in constant '{name}' might lead to unexpected behavior", cur_ln_num, self.file_n)
        var_type = self.parse_assignment_expression(line, cur_ln_num)
        if var_type == -1:
            return -1
        if var_type != () and self.is_valid_type(var_type, ("list",)):
            if isinstance(self.ast.cur_node.value, ArrayNode):
                self.ast.cur_node.children_types = tuple(set([child.type for child in self.ast.cur_node.value.children]))
            elif (self.ast.cur_node.value.__class__.__name__ == "FuncCallNode" and self.ast.cur_node.value.name != "range"):
                self.ast.cur_node.children_types = self.get_array_types()
            self.ast.cur_node.children_count = len(self.ast.cur_node.children_types)
        if self.ast.get_parent_node(IfNode, ElseNode, ElifNode) != -1 and self.ast.cur_node.first_define:
            self.ast.cur_node.conditional = True
            if self.ast.cur_node not in self.special_globals:
                self.special_globals.append(self.ast.cur_node)
        if self.ast.get_parent_node(ForLoopNode, WhileLoopNode) != -1 and self.ast.cur_node.first_define:
            self.ast.cur_node.loop_define = True
            if self.ast.cur_node not in self.special_globals:
                self.special_globals.append(self.ast.cur_node)
        self.ast.detraverse_node()
        return 0
    
    #################################Expression Parsing###############################

    def parse_expression(self, tokens: list[Token], traversal_type: str, ln_num: int, **exclude: bool) -> tuple[str] | int:
        """
        Parses an expression represented by token into its AST Node

        Returns -1 on error, otherwise the type of the expression that has been parsed
        """
        tokens = [token for token in tokens if token.token_t != "TT_eol"]
        no_tokens = True if not tokens else False
        if not no_tokens and tokens[0].token_t == "TT_lparen" and tokens[-1].token_t == "TT_rparen":
            tokens = tokens[1:-1]
        if not tokens or no_tokens:
            self.error = SyntaxError("Invalid Expression", ln_num, self.file_n)
            return -1
        cur_expr_map = EXPR_MAP.copy()
        for kw in exclude.keys():
            kw_int = int(kw[-1])
            if kw_int not in EXPR_MAP:
                raise Exception("Invalid Argument for function 'parse_expression': Argument must be in scope of EXPR_MAP!")
            cur_expr_map[kw_int] = False
        if cur_expr_map[0] and len(tokens) == 1:
            return self.parse_simple_literal_and_var(tokens[0], traversal_type)
        if cur_expr_map[1] and self.is_array_literal(tokens):
            return self.parse_array_literal(tokens, traversal_type)
        if len(tokens) >= 3 and cur_expr_map[3] and self.is_array_var(tokens):
            return self.parse_array_var(tokens, traversal_type)
        if len(tokens) >= 3 and cur_expr_map[5] and tokens[0].token_t == "TT_identifier" and tokens[1].token_t == "TT_lparen" and tokens[len(tokens)-1].token_t == "TT_rparen" and self.is_func_call(tokens):
            if self.parse_func_call(tokens, traversal_type, from_expr=True) == -1: return -1
            return self.identifier_manager.get_identifier_node(tokens[0].token_v).return_type
        if cur_expr_map[6] and tokens[0].token_t in ("TT_sub", "TT_not", "TT_plus"):
            return self.parse_un_op_expression(tokens, traversal_type, ln_num)
        for i, token in enumerate(tokens):
            if i == 0 or i == len(tokens)-1:
                continue
            if cur_expr_map[8] and token.token_t in ("TT_equ", "TT_less", "TT_greater", "TT_and", "TT_or", "TT_dequ", "TT_gequ", "TT_lequ", "TT_nequ") or (token.token_t in ("TT_equ", "TT_less", "TT_greater") and tokens[i+1] == "TT_equ"):
                return self.parse_conditional_expression(tokens, traversal_type)
            if cur_expr_map[7] and token.token_t in ("TT_sub", "TT_plus", "TT_mul", "TT_div", "TT_mod") and not [token for token in tokens if token.token_t in ("TT_equ", "TT_greater", "TT_less")]:
                return self.parse_binop_expression(tokens, traversal_type)
        if cur_expr_map[4] and [token for token in tokens if token.token_t == "TT_colon"]:
            return self.parse_slice_expression(tokens, traversal_type, ln_num)
        self.error = SyntaxError("Invalid Expression", ln_num, self.file_n)
        return -1

    def parse_assignment_expression(self, tokens: list[Token], ln_num: int) -> tuple[str] | int:
        """
        Parses an assignment expression in token representation into its AST Node

        Return -1 on error, otherwise the type of the value that has been assigned
        """
        name = tokens[0].token_v
        tokens = self.merge_equ(tokens)
        operator, operator_idx = self.get_operator_info(tokens, ASSIGNMENT_OPERATOR_PRECEDENCE_DICT)
        if operator.token_t in ["TT_pequ", "TT_sequ", "TT_dvequ", "TT_mequ", "TT_modequ"]:
            tokens = self.handle_op_assign(tokens, operator, operator_idx)
        left = tokens[:operator_idx]
        right = tokens[operator_idx+1:]
        new_node_id = self.ast.append_node(AssignNode(name, tokens[0].ln))
        self.ast.traverse_node_by_id(new_node_id)
        if self.identifier_manager.identifier_exists(left[0].token_v):
            old_def = self.identifier_manager.get_identifier_node(left[0].token_v)
            self.ast.cur_node.first_define = False
        else:
            self.ast.cur_node.first_define = True
            self.identifier_manager.set_identifier(left[0].token_v, self.ast.cur_node)
        expr_type = self.parse_expression(right, "value", ln_num, expr_4=False)
        self.ast.cur_node.type = expr_type
        if expr_type == -1: return -1
        if self.parse_expression(left, "left_expr", ln_num, expr_1=False, expr_2=False, expr_4=False, expr_5=False, expr_6=False, expr_7=False, expr_8=False) == -1: 
            return -1
        if self.ast.cur_node.left_expr.__class__.__name__ == "ArrayVarNode":
            if self.ast.cur_node.first_define:
                self.error = SyntaxError("Invalid Syntax", ln_num, self.file_n)
                return -1
            if not self.is_valid_type(old_def.type, ("list", "str")):
                self.error = TypeError("Cannot index non-container type", ln_num, self.file_n)
                return -1
        self.identifier_manager.set_identifier(self.identifier_manager.get_relative_identifier(left[0].token_v), self.ast.cur_node)

        return expr_type

    def parse_slice_expression(self, tokens: list[Token], traversal_type: str, ln_num: int) -> tuple[str] | int:
        """
        Takes the tokens representation of a slice expression and parses it into its AST Node 

        Returns -1 on error, otherwise ("int",) as slice expressions can only be of type int
        """
        if len(tokens) == 1:
            self.error = SyntaxError("Invalid syntax", tokens[0].token_t, self.file_n)
            return -1
        operator, operator_idx = self.get_operator_info(tokens, SLICE_OPERATOR_PRECEDENCE_DICT, is_slice_expr = True)
        if operator == -1: return -1
        new_node_id = self.ast.append_node(SliceExpressionNode(tokens[0].ln), traversal_type)
        self.ast.traverse_node_by_id(new_node_id, traversal_type)
        if operator_idx == len(tokens)-1 or operator_idx == 0:
            if operator_idx == 0:
                tokens_to_parse = tokens[1:]
                traversal_type = "right"
            else:
                tokens_to_parse = tokens[:len(tokens)-1]
                traversal_type = "left"
            expr_type = self.parse_expression(tokens_to_parse, traversal_type, ln_num)
        else:
            left = tokens[:operator_idx]
            right = tokens[operator_idx+1:]
            expr_type = self.parse_sides(left, right, ln_num, SLICE_EXPRESSION_ALLOWED_TYPES, ":")
        if expr_type == -1: return -1
        if not self.is_valid_type(expr_type, SLICE_EXPRESSION_ALLOWED_TYPES):
            self.error = TypeError(f"Invalid type for slice expression: '{expr_type}'", tokens[0].token_t, self.file_n)
            return -1
        self.ast.detraverse_node()
        return expr_type

    def parse_conditional_expression(self, tokens: list[Token], traversal_type: str) -> str | int:
        """
        Takes the token representation of a conditional expression and parses it into its AST Node

        Returns -1 on error, otherwise the type of the expression
        """
        if not [token for token in tokens if token.token_t in ("TT_dequ", "TT_gequ", "TT_jequ")]:
            tokens = self.merge_equ(tokens)
        operator, operator_idx = self.get_operator_info(tokens, CONDITION_OPERATOR_PRECEDENCE_DICT)
        if operator == -1: return -1
        operator_str = get_token_ident(operator.token_t)
        left = tokens[:operator_idx]
        right = tokens[operator_idx+1:]
        new_node_id = self.ast.append_node(ConditionExpressionNode(operator_str, tokens[0].ln), traversal_type)
        self.ast.traverse_node_by_id(new_node_id, traversal_type)
        if operator.token_t in ("TT_and", "TT_or"):
            allowed_types = CONDITIONAL_EXPRESSION_LOGICAL_OPERATOR_ALLOWED_TYPES
        else:
            allowed_types = CONDITIONAL_EXPRESSION_CONDTIONAL_OPERATOR_AllOWED_TYPES
            if operator_str in ["==", "!="]:
                allowed_types += CONDITIONAL_EXPRESSION_DEQU_ALLOWED_TYPES
        if self.parse_sides(left, right, tokens[0].ln, allowed_types, operator_str) == -1: return -1
        self.ast.cur_node.type = ("bool",)
        self.ast.detraverse_node()
        return ("bool",)
        
    def parse_binop_expression(self, tokens: list[Token], traversal_type: str) -> str | int:
        """
        Takes the token representation of a binary operator expression and parses it into its AST Node

        Returns -1 on error, otherwise the type of the expression
        """
        operator, operator_index = self.get_operator_info(tokens, BINOP_OPERATOR_PRECEDENCE_DICT)
        if operator == -1: return -1
        allowed_types = BINOP_EXPRESSION_BASE_ALLOWED_TYPES
        if operator.token_t == "TT_plus":
            allowed_types += BINOP_EXPRESSION_PLUS_ALLOWED_TYPES
        if operator.token_t == "TT_mul":
            allowed_types += BINOP_EXPRESSION_MUL_ALLOWED_TYPES
        if operator.token_t ==  "TT_mod":
            allowed_types = BINOP_EXPRESSION_MOD_ALLOWED_TYPES
        left = tokens[:operator_index]
        right = tokens[operator_index+1:]
        new_node_id = self.ast.append_node(BinOpNode(get_token_ident(operator.token_t), tokens[0].ln), traversal_type)
        self.ast.traverse_node_by_id(new_node_id, traversal_type)
        expr_type = self.parse_sides(left, right, tokens[0].ln, allowed_types, get_token_ident(operator.token_t))
        if expr_type == -1: return -1    
        self.ast.cur_node.type = expr_type
        self.ast.detraverse_node()
        return expr_type
    
    def get_operator_info(self, tokens: list[Token], operator_precedence_dict: dict[str: int], is_slice_expr: bool = False) -> tuple[str | int, int]:
        """
        Selects the most precedented operator in an expression using the operator precedence dict and information about parenthesis

        Returns (-1, -1) on error, otherwise the found operator and its index
        """
        operator_precedence = operator_precedence_dict
        best_operator = None
        paren_depth = 0
        bracket_depth = 0
        for i, token in enumerate(tokens):
            if token.token_t in ("TT_lparen", "TT_lbracket", "TT_rparen", "TT_rbracket"):
                match token.token_t:
                    case "TT_lparen":
                        paren_depth += 1
                    case "TT_lbracket":
                        bracket_depth += 1
                    case "TT_rparen":
                        paren_depth -= 1
                    case "TT_rbracket":
                        bracket_depth -= 1
                continue
            if paren_depth or bracket_depth or token.token_t not in operator_precedence.keys():
                continue
            if (i == 0 or i == len(tokens)-1) and not is_slice_expr:
                self.error = SyntaxError("Invalid Syntax", tokens[0].ln, self.file_n)
                return -1, -1
            if operator_precedence[token.token_t] == 2:
                if not best_operator:
                    best_operator = token
                    continue
            best_operator = token
            break
        if not best_operator or paren_depth or bracket_depth:
            self.error = SyntaxError("Invalid Syntax", tokens[0].ln, self.file_n)
            return -1, -1
        return best_operator, tokens.index(best_operator)
    
    def parse_sides(self, left: list[Token], right: list[Token], ln_num: int, allowed_types: list[tuple[str, str]], operator: str) -> str | int:
        """
        Parses the left and right sides of an expression to their AST Nodes and infers the type of the result of the expression if possible
    
        Returns -1 on error, otherwise the type of the expression
        """
        left_expr_type = self.parse_expression(left, "left", ln_num, expr_4=False)
        if left_expr_type == -1: return -1
        right_expr_type = self.parse_expression(right, "right", ln_num, expr_4=False)
        if right_expr_type == -1: return -1
        expr_type = self.resolve_types(left_expr_type, right_expr_type, operator, ln_num)
        if expr_type == -1: return -1
        if not self.is_valid_type(expr_type, allowed_types):
            self.error = TypeError("Invalid Expression type", ln_num, self.file_n)
            return -1
        return expr_type

    def parse_un_op_expression(self, tokens: list[Token], traversal_type: str, ln_num) -> str | int:
        """
        Parses the token representation of a unary operator expression to its AST Node

        Returns -1 on error, otherwise the type of the expression
        """
        operator = UNOP_OPERATOR_DICT[tokens[0].token_t]
        if operator == "+":
            return self.parse_expression(tokens[1:], traversal_type, ln_num)
        node_id = self.ast.append_node(UnOpNode(operator, tokens[0].ln), traversal_type)
        self.ast.traverse_node_by_id(node_id, traversal_type)
        expr_type = self.parse_expression(tokens[1:], "right", tokens[0].ln, expr_1=False, expr_3=False, expr_4=False, expr_6=False)
        if operator == "-" and not self.is_valid_type(expr_type, UNOP_EXPRESSION_SUB_ALLOWED_TYPES):
            self.error = TypeError(f"Bad type for unary operator '-': {expr_type}", tokens[0].ln, self.file_n)
            return -1
        if operator == "not" and not self.is_valid_type(expr_type, UNOP_EXPRESSION_NOT_ALLOWED_TYPES):
            self.error = TypeError(f"Bad type for unary operator 'not': {expr_type}", tokens[0].ln, self.file_n)
            return -1
        self.ast.detraverse_node()
        return expr_type
        
    def parse_array_var(self, tokens: list[Token], traversal_type: str) -> str | int:
        """
        Parses an array indexing expression represented by tokens into its AST Node

        Returns -1 on error, otherwise ()
        """
        cur_ln_num = tokens[0].ln
        var_identifier = tokens[0].token_v
        if not self.identifier_manager.identifier_exists(var_identifier):
            self.error = NameError(f"Name {var_identifier} is not defined", cur_ln_num, self.file_n)
            return -1
        var_dec_node = self.identifier_manager.get_identifier_node(var_identifier)
        self.ast.cur_node.child_count = var_dec_node.child_count
        if not self.is_valid_type(var_dec_node.type, ("str", "list")):
            self.error = TypeError(f"{var_dec_node.type} object is not subscriptable", cur_ln_num, self.file_n)
            return -1
        content_exprs = self.get_content_expressions(tokens[1:])
        if content_exprs == -1: return -1
        tokens = tokens[2:len(tokens)-1]
        new_node_id = self.ast.append_node(ArrayVarNode(var_identifier, tokens[0].ln), traversal_type)
        self.ast.traverse_node_by_id(new_node_id, traversal_type)
        for content_expr in content_exprs:
            expr_type = self.parse_expression(content_expr, "content", cur_ln_num, expr_1=False, expr_3=False, expr_8=False)
            if expr_type == -1: return -1
            if not self.is_valid_type(expr_type, ("int",)):
                self.error = TypeError(f"list indices must be of type int, not {expr_type}", cur_ln_num, self.file_n)
                return -1 
        self.ast.detraverse_node()
        return () #Return empty tuple to mark variable as any type as empty tuple will pass all is_valid_type() checks
    
    def get_content_expressions(self, tokens: list[Token]) -> list[list[Token]] | int:
        """
        Parses the content of a list literal expression into a list of the element expressions

        Returns -1 on error, otherwise the list of the content expressions
        """
        content_expressions = []
        cur_content_expr = []
        bracket_depth = 0
        for token in tokens:
            if token.token_t == "TT_lbracket": 
                bracket_depth += 1
                continue
            if token.token_t == "TT_rbracket":
                if bracket_depth == 0:
                    self.error = SyntaxError("']' was never opened", token.ln, self.file_n)
                    return -1
                if bracket_depth == 1:
                    content_expressions.append(cur_content_expr)
                    cur_content_expr = []
                bracket_depth -= 1
                continue
            if bracket_depth == 0 and token.token_t != "TT_lbracket":
                self.error = SyntaxError("Unexpected Token", token.ln, self.file_n)
                return -1
            cur_content_expr.append(token)
        if bracket_depth:
            self.error = SyntaxError("'[' was never closed", tokens[0].ln, self.file_n)
            return -1

        return content_expressions

        
    def parse_array_literal(self, tokens: list[Token], traversal_type: str) -> str | int:
        """
        Parses a list literal expression represented by tokens into its AST Node

        Returns -1 on error, otherwise ("list",)
        """
        tokens = tokens[1:len(tokens)-1]
        if tokens == []:
            self.ast.append_node(ArrayNode(), traversal_type)
            return ("list",)
        arr_element_expressions = []
        cur_arr_element_expression = []
        paren_depth = 0
        for token in tokens:
            if token.token_t in ("TT_lparen", "TT_lbracket"):
                paren_depth += 1
            if token.token_t in ("TT_rparen", "TT_rbracket"):
                if paren_depth == 0:
                    self.error = SyntaxError("Invalid Syntax", tokens[0].token_t, self.file_n)
                    return -1
                paren_depth -= 1
            if token.token_t == "TT_comma" and not paren_depth:
                if cur_arr_element_expression:
                    arr_element_expressions.append(cur_arr_element_expression)
                    cur_arr_element_expression = []
                    continue
                else:
                    self.error = SyntaxError("Invalid Syntax", token.ln, self.file_n)
                    return -1
            cur_arr_element_expression.append(token)
        if not cur_arr_element_expression:
            self.error = SyntaxError("Invalid Syntax", token.ln, self.file_n)
            return -1
        arr_element_expressions.append(cur_arr_element_expression)
        node_id = self.ast.append_node(ArrayNode(), traversal_type)
        self.ast.traverse_node_by_id(node_id, traversal_type)
        for arr_element_expression in arr_element_expressions:
            if self.parse_expression(arr_element_expression, "children", tokens[0].ln, expr_4=False) == -1: return -1
        self.ast.detraverse_node()
        return ("list",)
        
    def parse_simple_literal_and_var(self, token: Token, traversal_type: str) -> str | int:
        """
        Parses the token representation of a simple literal or an identifier into its AST Node

        Returns -1 on error, otherwise the type of the variable or literal
        """
        if token.token_t in ("TT_int", "TT_float"):
            new_node_id = self.ast.append_node(NumberNode(token.token_v), traversal_type)
            self.ast.traverse_node_by_id(new_node_id, traversal_type)
            self.ast.cur_node.type = token.token_t[3:]
            self.ast.cur_node.line = token.ln
            self.ast.detraverse_node()
            return (token.token_t[3:],)
        if token.token_t == "TT_bool":
            new_node_id = self.ast.append_node(BoolNode(token.token_v), traversal_type)
            self.ast.traverse_node_by_id(new_node_id, traversal_type)
            self.ast.cur_node.line = token.ln
            self.ast.detraverse_node()
            return ("bool",)
        if token.token_t == "TT_str":
            self.ast.append_node(StringNode(token.token_v), traversal_type)
            return ("str",)
        if token.token_t == "TT_none":
            self.ast.append_node(NoneNode(), traversal_type)
            return ("none",)
        if token.token_t == "TT_identifier":
            name = self.identifier_manager.get_relative_identifier(token.token_v)
            if not self.identifier_manager.identifier_exists(name):
                self.error = NameError(f"Unknown Identifier: '{name}'", token.ln, self.file_n)
                return -1
            if name in self.identifier_manager.get_func_identifier_dict():
                self.error = TypeError("Higher Order functions are not supported. You might be overwriting a built-in function name.", token.ln, self.file_n)
                return -1
            new_node_id = self.ast.append_node(VarNode(name), traversal_type)
            self.ast.traverse_node_by_id(new_node_id, traversal_type)
            cur_node_type = self.identifier_manager.get_identifier_node(name).type
            self.ast.cur_node.type = cur_node_type
            self.ast.cur_node.line = token.ln
            self.ast.detraverse_node()
            return cur_node_type
            
        self.error = SyntaxError("Invalid Literal", token.ln, self.file_n)
        return -1
    
    ########################################Misc########################################
    
    def resolve_types(self, left_expr_type: tuple[str], right_expr_type: tuple[str], operator: str, ln_num: int) -> tuple[str] | int:
        """
        Inferes the type of a binary operator expression (or conditional expression) based on the types of both sides of the expression and the expression operator

        Returns -1 on error, otherwise a tuple representing the possible types of the expression
        """
        if left_expr_type == None or right_expr_type == None or () in left_expr_type or () in right_expr_type:
            return ()
        types = []
        if operator in ("==", "!="):
            return ("bool",)
        if left_expr_type == right_expr_type:
            if not self.check_type_exceptions(left_expr_type, right_expr_type, operator, ln_num):
                return -1
            return left_expr_type if isinstance(left_expr_type, tuple) else (left_expr_type,)
        if len(left_expr_type) == len(right_expr_type) and len(left_expr_type) == 1:
            expr_type = (left_expr_type[0], right_expr_type[0])
            if self.is_valid_type(expr_type, ("int", "str")):
                if operator == "*": return ("str",)
                self.error = TypeError("Invalid combination of types 'int', 'str'", ln_num, self.file_n)
                return -1
            if self.is_valid_type(expr_type, ("int", "list")):
                if operator == "*": return ("list",)
                self.error = TypeError("Invalid combination of types 'int', 'list'", ln_num, self.file_n)
                return -1
            if self.is_valid_type(expr_type, ("list", "str")) and operator == "*":
                return ("list",)
            if self.is_valid_type(expr_type, ("float", "int")): 
                return ("float",)
            self.error = TypeError(f"Invalid combination of types: '{expr_type[0]}', '{expr_type[1]}'", ln_num, self.file_n)
            return -1
        combinations = get_combinations(left_expr_type, right_expr_type)
        for combination in combinations:
            combination_type = self.resolve_types((combination[0],), (combination[1],), operator, ln_num)
            if combination_type == -1: return -1
            types += list(combination_type)
        return tuple(set(types))
    
    def check_type_exceptions(self, left_expr_type: tuple[str], right_expr_type: tuple[str], operator: str, line: int) -> bool:
        if operator == "*" and self.is_valid_type(left_expr_type, ("list", "str")) and left_expr_type != () and right_expr_type != ():
            self.error = TypeError(f"Invalid combination of types: '{left_expr_type[0]}', '{right_expr_type[0]}'", line, self.file_n)
            return False
        return True
                

    def is_valid_type(self, t1: tuple[str], t2: tuple[str]) -> bool:
        """
        Compares two tuples representing the types of two values and compares them, returning True when the first tuple is a sublist of the second tuple
        
        Returns a boolean value
        """
        
        if t1 == None:
            return True
        if len(t1) > len(t2):
            return False
        if len(t1) > 1:
            t1 = tuple(sorted(t1))
        if len(t2) > 1:
            t2 = tuple(sorted(t2))
        if t1 not in get_sublists(t2):
            return False
        return True

    def is_array_literal(self, tokens: list[Token]) -> bool:
        """
        Checks if a group of tokens represents a list literal expression

        Returns a boolean value
        """
        if tokens[0].token_t != "TT_lbracket" or tokens[-1].token_t != "TT_rbracket":
            return False
        bracket_depth = 0
        for i, token in enumerate(tokens):
            match token.token_t:
                case "TT_lbracket":
                    bracket_depth += 1
                case "TT_rbracket":
                    if bracket_depth != 1 or i == len(tokens)-1:
                        bracket_depth -= 1
                    else:
                        return False
        return True

    def is_array_var(self, tokens: list[Token]) -> bool:
        """
        Checks if a group of tokens represents an array indexing expression

        Returns a boolean value
        """
        if tokens[0].token_t != "TT_identifier": return False
        tokens = tokens[1:]
        bracket_depth = 0
        for token in tokens:
            if token.token_t == "TT_lbracket":
                bracket_depth += 1
            if token.token_t == "TT_rbracket":
                if bracket_depth == 0: return False
                bracket_depth -= 1
                if bracket_depth == 0 and tokens.index(token) == len(tokens) -1: return True
            if bracket_depth == 0 and token.token_t != "TT_lbracket": return False

        return True
    
    def get_array_types(self) -> list[str]:
        """
        Inferes the types of the elements of an ArrayNode, being the current node.

        Returns -1 on error, otherwise the types of the children of the ArrayNode making up the current node inside the ast
        """
        cur_node = self.ast.cur_node
        array_nodes = self.get_iter_nodes()
        self.ast.cur_node = cur_node
        if array_nodes == -1: return -1
        if isinstance(array_nodes, tuple):
            return array_nodes
        types = []
        for array_node in array_nodes:
            if array_node.children:
                for child in array_node.children:
                    types.append(child.type)
        return tuple(set(types))
    
    def get_iter_nodes(self) -> list[ASTNode] | tuple[str]:
        """
        Recursively searches all nodes making up the ArrayNode in self.ast.cur_node and returns them.
        May also return the inferred types directly
        """
        if isinstance(self.ast.cur_node, ForLoopNode):
            if isinstance(self.ast.cur_node.iter, ArrayNode):
                return [self.ast.cur_node.iter]
            self.ast.traverse_node("iter")
            return self.get_iter_nodes()
        if isinstance(self.ast.cur_node, ArrayNode):
            return [self.ast.cur_node]
        if isinstance(self.ast.cur_node, AssignNode):
            if not self.ast.cur_node.value:
                parent_func_def_node = self.ast.get_parent_node(FuncDefNode)
                if not parent_func_def_node.func_call_nodes:
                    return []
                arg_index = parent_func_def_node.arg_names.index(self.ast.cur_node.name)
                self.ast.cur_node = parent_func_def_node.func_call_nodes[0].args[arg_index]
                return self.get_iter_nodes()
            if self.ast.cur_node.children_types:
                return self.ast.cur_node.children_types
            else:
                self.ast.traverse_node("value")
                return self.get_iter_nodes()
        if isinstance(self.ast.cur_node, VarNode):
            self.ast.cur_node = self.identifier_manager.get_identifier_node(self.ast.cur_node.name)
            return self.get_iter_nodes()
        if isinstance(self.ast.cur_node, BinOpNode):
            cur_node = self.ast.cur_node
            self.ast.traverse_node("right")
            right_types = self.get_iter_nodes()
            if right_types == -1: return -1
            self.ast.cur_node = cur_node
            return right_types
        if isinstance(self.ast.cur_node, FuncCallNode):
            if self.ast.cur_node.name == "range": return ()
            self.ast.cur_node = self.identifier_manager.get_identifier_node(self.ast.cur_node.name)
            nodes = []
            cur_node = self.ast.cur_node
            for node in self.ast.cur_node.return_nodes:
                self.ast.cur_node = node
                self.ast.traverse_node("return_value")
                nodes += self.get_iter_nodes()
            self.ast.cur_node = cur_node
            return nodes
        return []

    def merge_equ(self, tokens: list[Token]) -> list[Token]:
        """
        Takes a list of tokens representing an expression and merges operator tokens neighboring an equ token to one token

        Returns the new list of tokens with the merged operators
        """
        tokens_merged_equ = []
        i = 0
        while i < len(tokens):
            token = tokens[i]
            if i == len(tokens)-1:
                tokens_merged_equ.append(token)
                break
            next_token = tokens[i+1]
            if token.token_t not in ("TT_equ", "TT_greater", "TT_less", "TT_exclam", "TT_plus", "TT_sub", "TT_mul", "TT_div", "TT_mod"):
                tokens_merged_equ.append(token)
                i += 1
                continue
            if next_token.token_t != "TT_equ":
                tokens_merged_equ.append(token)
                i += 1
                continue
            match token.token_t:
                case "TT_equ": token.token_t = "TT_dequ"
                case "TT_greater": token.token_t = "TT_gequ"   
                case "TT_less": token.token_t = "TT_lequ"
                case "TT_exclam": token.token_t = "TT_nequ"
                case "TT_plus": token.token_t = "TT_pequ"
                case "TT_sub": token.token_t = "TT_sequ"
                case "TT_mul": token.token_t = "TT_mequ"
                case "TT_div": token.token_t = "TT_dvequ"
                case "TT_mod": token.token_t = "TT_modequ"

            tokens_merged_equ.append(token)
            i += 2
        return tokens_merged_equ
    
    def get_children(self, child_line_tokens: list[Token], parent_line_indentation: int) -> int:
        """
        Obtains the tokens making up the body of a statement containing an indented block

        Returns -1 on error, otherwise the amount of lines in the statement body and the lines of tokens representing the body
        """
        if not child_line_tokens:
            self.error = SyntaxError("Expected Block after statement", self.tokens[-1].ln-1, self.file_n)
            return -1
        cur_indentation = None
        block_to_parse = []
        for line in child_line_tokens:
            if line[0].token_t != "TT_pind":
                break
            pind_token = line[0]
            if not self.indentation:
                self.indentation = pind_token.token_v
            if not cur_indentation:
                if pind_token.token_v == parent_line_indentation + self.indentation:
                    cur_indentation = pind_token.token_v
                else:
                    self.error = IndentationError("Indent does not match previous Indents", pind_token.ln, self.file_n)
                    return -1
            if pind_token.token_v < cur_indentation:
                if pind_token.token_v % self.indentation == 0:
                    break
                self.error = IndentationError("Unindent does not match previous Indent", line[0].ln, self.file_n)
            block_to_parse.append(line)
        if block_to_parse == []:
            self.error = IndentationError("Expected Indent", line[0].ln, self.file_n)
            return -1
        children_count = len(block_to_parse)
        return children_count, block_to_parse
    
    def get_indentation(self, pind_token: Token) -> int:
        """
        Obtains the indentation level of a block based on the pind (positive indent) token

        Returns -1 on error, otherwise the current indentation in spaces
        """
        if pind_token.token_t != "TT_pind":
            return 0
        if not self.cur_block_indentation:
            self.cur_block_indentation = pind_token.token_v
        if pind_token.token_v != self.cur_block_indentation:
            self.error = IndentationError("Unexpected Indent", pind_token.ln, self.file_n)
            return -1
        return pind_token.token_v
    
    def get_line_tokens(self, tokens: list[Token]) -> list[list[Token]]:
        """
        Groups the tokens representing a block into their respective lines, represented in a 2-dimensional list

        Returns the newly grouped tokens
        """
        if not tokens: return [tokens]
        line_tokens = []
        highest_line_number = tokens[len(tokens)-1].ln
        for i in range(1, highest_line_number+1):
            line = [token for token in tokens if token.ln == i and token.token_t not in ("TT_squote", "TT_dquote")]
            if not line or len(line) < 2 or (line[0].token_t == "TT_pind" and line[1].token_t == "TT_eol"): continue
            line_tokens.append(line)
        return line_tokens
    
    def var_args_checking(self, name: str, arg_types: tuple, cur_ln_num: int) -> bool:
        """
        Implements variable argument count functionality, checking the types of arguments for built-in functions

        TODO: Refactor

        Returns a boolean value representing whether the arguments given to the built-in function are valid
        """
        if name not in VAR_ARG_BUILT_IN_FUNCS:
            return False
        if name == "strip":
            if len(arg_types) == 1:
                if self.is_valid_type(arg_types[0], ("str",)): return True
                else:
                    self.error = TypeError("Invalid argument type for function strip()", cur_ln_num, self.file_n)
                    return False
            if len(arg_types) == 2:
                if self.is_valid_type(arg_types[0], ("str",) and self.is_valid_type(arg_types[1], ("str",))): return True
                else:
                    self.error = TypeError("Invalid argument type for function strip()", cur_ln_num, self.file_n)
                    return False
            return False
        if name == "range":
            if len(arg_types) < 1 or len(arg_types) > 3:
                return False
            for arg_type in arg_types:
                if not self.is_valid_type(arg_type, ("int",)):
                    self.error = TypeError("Invalid argument type for function range()", cur_ln_num, self.file_n)
                    return False
            return True
        if name == "input":
            if len(arg_types) > 1:
                return False
        if name == "exit":
            if len(arg_types) > 1:
                return False
            if arg_types and not self.is_valid_type(arg_types[0], ("int",)):
                return False
        return True
    
    def is_func_call(self, tokens: list[Token]) -> bool:
        """
        Checks whether a group of tokens represents a function call expression

        Returns a boolean value
        """
        paren_depth = 0
        for token in tokens:
            match token.token_t:
                case "TT_lparen": paren_depth += 1
                case "TT_rparen":
                    if paren_depth > 0:
                        paren_depth -= 1
                    else:
                        return False
            if not paren_depth and token.token_t in OPERATORS:
                return False

        return True
    
    def is_assign(self, line: list[Token]) -> bool:
        """
        Checks whether a group of tokens represents an assignment expression

        Returns a boolean value
        """
        line_copy = deepcopy(line)
        line_copy = self.merge_equ(line_copy)
        if line_copy[0].token_t == "TT_identifier" and line_copy[1].token_t in ["TT_pequ", "TT_sequ", "TT_mequ", "TT_dvequ", "TT_modequ", "TT_equ"]:
            return True
        if not [token for token in line_copy if token.token_t == "TT_equ"]:
            return False
        operator_idx = self.get_operator_info(line_copy, ASSIGNMENT_OPERATOR_PRECEDENCE_DICT)[1]
        left = line_copy[:operator_idx]
        return self.is_array_var(left)
    
    def handle_op_assign(self, tokens: list[Token], operator: str, operator_idx: int) -> list[Token]:
        match operator.token_t:
            case "TT_pequ": op_token = Token(tokens[0].ln, 0, "TT_plus", None)
            case "TT_sequ": op_token = Token(tokens[0].ln, 0, "TT_sub", None)
            case "TT_mequ": op_token = Token(tokens[0].ln, 0, "TT_mul", None)
            case "TT_dvequ": op_token = Token(tokens[0].ln, 0, "TT_div", None)
            case "TT_modequ": op_token = Token(tokens[0], 0, "TT_mod", None)
        
        tokens[operator_idx].token_t = "TT_equ"
        assign_identifier_token = tokens[0]
        tokens.insert(operator_idx+1, assign_identifier_token)
        tokens.insert(operator_idx+2, op_token)

        return tokens
