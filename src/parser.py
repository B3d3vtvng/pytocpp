"""
Parser for the pytoc Python transpiler. 
"""

from copy import deepcopy

from src.utils.error import *
from src.utils.header import INVALID_VAR_NAMES, INVALID_FUNC_NAMES, STDLIB_TO_MODULE_PATH_DICT
from src.utils.operators import BINOP_OPERATOR_PRECEDENCE_DICT, CONDITION_OPERATOR_PRECEDENCE_DICT, SLICE_OPERATOR_PRECEDENCE_DICT, ASSIGNMENT_OPERATOR_PRECEDENCE_DICT, EXPR_MAP, OPERATORS, UNOP_OPERATOR_DICT
from src.utils.tokens import Token
from src.utils.list_util_funcs import get_sublists, get_combinations
from src.utils.allowed_type_constants import *
from src.utils.built_in_funcs import BUILT_IN_FUNC_DICT, BUILT_IN_FUNC_NAMES, VAR_ARG_BUILT_IN_FUNCS
from src.identifier_manager import IdentifierManager, IdentifierContainer
from src.nodes import *
from src.lexer import get_token_ident

class Parser():
    def __init__(self, tokens: list[Token], file_n: str, flags: dict[str: str | None]) -> None:
        """
        Attributes:

        error: Main location for all errors, all errors are 
        stored in this variable

        tokens: Tokens generated by the parser

        file_n: Target file name

        func_identifier_dict: Dictionary containing all function identifiers
        and their respective FuncDefNode

        var_identifier_dict: Same as func_identifier_dict but for 
        variables and only for the global scope

        indentation: First indentation amount used by the user, 
        error is thrown if other amount is used

        cur_block_indentation: The indentation amount of the
        block currently processed by parse_block()

        ast: Main instance of AST(), later returned by make_ast()
        """
        self.error = None
        self.warning = None
        self.tokens = tokens
        self.file_n = file_n
        self.flags = flags
        self.indentation = None
        self.cur_block_indentation = None
        self.ast = AST()
        self.identifier_manager = IdentifierManager(self.ast)
        self.ast_init()

    def ast_init(self) -> None:
        self.ast.append_node(AssignNode("argc"))
        self.ast.base_node.children[0].type = ("int",)
        self.ast.base_node.children[0].id = -1
        self.ast.append_node(AssignNode("argv"))
        self.ast.base_node.children[1].type = ("list",)
        self.ast.base_node.children[1].id = -2
        self.ast.next_free_id = 1
        self.identifier_manager.set_identifier("argc", self.ast.base_node.children[0])
        self.identifier_manager.set_identifier("argv", self.ast.base_node.children[1])

    def make_ast(self) -> AST:
        """
        Calls parse_block() with tokens, returns -1 on error, 
        otherwise the ast
        """
        if self.parse_block(self.tokens) == -1:
            return None
        self.ast.base_node.children = self.ast.base_node.children[2:]
        
        return self.ast, self.identifier_manager
    
    #################################General Parsing######################################
    
    def parse_block(self, tokens: list[Token]) -> None:
        """
        Takes a list of tokens, splits it into their respective lines
        and calls parse_line() on each line, taking indented blocks
        and function definitions into consideration by i being adjusted
        after every call to parse_line(), skipping indented blocks

        Returns -1 on error, otherwise 0
        """
        line_tokens = self.get_line_tokens(tokens)
        i = 0
        while i < len(line_tokens):
            if line_tokens[i][0].token_t == "TT_eof": break
            i = self.parse_line(i, line_tokens[i], line_tokens[i+1:])
            if i == -1:
                return i
        self.cur_block_indentation = None
        return 0
    
    def parse_line(self, i: int, line: list[Token], rem_line_tokens: list[list[Token]]) -> int:
        """
        Takes the current index of the current line in the array of all lines
        in parse_block(), the line to be parsed and the remaining lines
        to take care of indented blocks following the current line

        Returns i+1 on EOF, detects and validates the current indentation and
        calls different parsing functions depending on the tokens found in line

        Throws an error and returns -1 when given line doesnt match any statement
        """
        if len(line) == 1 and line[0].token_t == "TT_eof":
            return i+1
        
        cur_line_indentation = self.get_indentation(line[0])
        if cur_line_indentation == -1: return -1
        line = line[1:] if cur_line_indentation != 0 else line

        if self.is_assign(line):
            if self.parse_assign(line) == -1: return -1
            return i+1
        elif line[0].token_t == "TT_continue":
            if self.parse_continue_statement(line) == -1: return -1
            return i+1
        elif line[0].token_t == "TT_pass":
            if self.parse_pass_statement(line) == -1: return -1
            return i+1
        elif line[0].token_t == "TT_break":
            if self.parse_break_statement(line) == -1: return -1
            return i+1
        elif line[0].token_t == "TT_import":
            if self.parse_import_statement(line) == -1: return -1
            return i+1
        elif line[0].token_t == "TT_identifier" and line[1].token_t == "TT_lparen" and line[len(line)-2].token_t == "TT_rparen":
            if self.parse_func_call(line) == -1: return -1
            return i+1
        elif line[0].token_t in ("TT_if", "TT_elif", "TT_else"):
            line_count = self.parse_conditional_statement(line, rem_line_tokens, cur_line_indentation)
            if line_count == -1: return -1
            return i+line_count+1
        elif line[0].token_t == "TT_while":
            line_count = self.parse_while_loop(line, rem_line_tokens, cur_line_indentation)
            if line_count == -1: return -1
            return i+line_count+1
        elif line[0].token_t == "TT_for":
            line_count = self.parse_for_loop(line, rem_line_tokens, cur_line_indentation)
            if line_count == -1: return -1
            return i+line_count+1
        elif line[0].token_t == "TT_def":
            line_count = self.parse_func_def(line, rem_line_tokens, cur_line_indentation)
            if line_count == -1: return -1
            return i+line_count+1
        elif line[0].token_t == "TT_ret":
            if self.parse_return_statement(line) == -1: return -1
            return i+1
        
        self.error = SyntaxError("Invalid Syntax", line[0].ln, self.file_n)
        return -1
    
    ###########################Control Flow and Statement Parsing################################
    
    def parse_break_statement(self, line: list[Token]) -> int:
        cur_ln_num = line[0].ln
        line = line[:-1]
        if len(line) > 1:
            self.error = SyntaxError("Invalid Syntax", cur_ln_num, self.file_n)
            return -1
        
        new_node_id = self.ast.append_node(BreakNode())
        self.ast.traverse_node_by_id(new_node_id)

        parent_node = self.ast.get_parent_node((ForLoopNode, WhileLoopNode))
        if not parent_node:
            self.error = SyntaxError("Continue Statement not properly inside loop", cur_ln_num, self.file_n)
            return -1

        self.ast.detraverse_node()

        return 0
    
    def parse_pass_statement(self, line: list[Token]) -> int:
        cur_ln_num = line[0].ln
        line = line[:-1]
        if len(line) > 1:
            self.error = SyntaxError("Invalid Syntax", cur_ln_num, self.file_n)
            return -1
        
        new_node_id = self.ast.append_node(PassNode())

        return 0
    
    def parse_continue_statement(self, line: list[Token]) -> int:
        cur_ln_num = line[0].ln
        line = line[:-1]
        if len(line) > 1:
            self.error = SyntaxError("Invalid Syntax", cur_ln_num, self.file_n)
            return -1
        
        new_node_id = self.ast.append_node(ContinueNode())
        self.ast.traverse_node_by_id(new_node_id)

        parent_node = self.ast.get_parent_node(ForLoopNode)
        if not parent_node:
            self.error = SyntaxError("Continue Statement not properly inside loop", cur_ln_num, self.file_n)
            return -1

        self.ast.detraverse_node()

        return 0
    
    def parse_import_statement(self, line: list[Token]) -> int:
        cur_ln_num = line[0].ln
        line = line[1:-1]
        module_path = self.get_module_path(line, cur_ln_num)
        if module_path == -1: return -1

        if module_path in STDLIB_TO_MODULE_PATH_DICT.keys():
            module_path = STDLIB_TO_MODULE_PATH_DICT[module_path]

        self.ast.append_node(ImportNode(module_path))

        from src.compiler import Compiler

        flags = deepcopy(self.flags)
        flags["--import"] = None

        import_compiler = Compiler(module_path, flags)
        import_ast, import_ident_man = import_compiler.compile()

        args = (self.ast.base_node.children.pop(0), self.ast.base_node.children.pop(0))
        self.ast.base_node.children = import_ast.base_node.children + self.ast.base_node.children
        for i in range(len(args)):
            self.ast.base_node.children.insert(i, args[i])
        self.ast.readjust_ids()

        self.identifier_manager.merge_identifiers(import_ident_man)

        return 0

    def get_module_path(self, line: list[Token], ln_num: int) -> str:
        req_identifier = True
        path_str = ""
        for token in line:
            if req_identifier:
                if token.token_t != "TT_identifier":
                    self.error = SyntaxError("Invalid Syntax", ln_num, self.file_n)
                    return -1
                path_str += token.token_v
            else:
                if token.token_t != "TT_dot":
                    self.error = SyntaxError("Invalid Syntax", ln_num, self.file_n)
                    return -1
                path_str += "/"
            req_identifier = not req_identifier
        
        if req_identifier:
            self.error = SyntaxError("Invalid Syntax", ln_num, self.file_n)
            return -1

        return path_str + ".py"


    def parse_return_statement(self, line: list[Token]) -> int:
        """
        Takes the line containing the return statement, 
        Verifies that the return statement is inside a function,
        Creates ReturnNode object inside the ast,
        Parses the expression representing its value,
        Adds information concerning its type into the ast

        Returns -1 on error, otherwise 0
        """
        cur_ln_num = line[0].ln
        line = line[1:]
        #Looks for FuncDefNode as parent node because then it would be in a function
        parent_node = self.ast.get_parent_node(FuncDefNode)
        if parent_node == -1:
            self.error = SyntaxError("Return Statement outside function!", cur_ln_num, self.file_n)
            return -1
        new_node_id = self.ast.append_node(ReturnNode())
        self.ast.traverse_node_by_id(new_node_id)
        return_type = self.parse_expression(line, "return_value", cur_ln_num, expr_4=False)
        if return_type == -1: return -1
        self.ast.cur_node.type = return_type
        parent_node.return_type = ()
        parent_node.return_nodes += [self.ast.cur_node]
        self.ast.detraverse_node()
        return 0
    
    def parse_func_def(self, line: list[Token], rem_line_tokens: list[list[Token]], cur_line_indentation: int) -> int:
        """
        Takes the line to be parsed, the remaining lines of tokens and the current indentation

        Verifies that the last token in the line is a colon, 
        Verifies that the def keyword is followed by an identifier being the function name,
        Verifies that the function name is followed by '(' and the second last token is a ')',
        Parses the argument names,
        Creates new FuncDefNode in the ast,
        Adds the new function to the func_identifier_dict
        Calls get_children() to obtain function body, saves unparsed body in node attribute 'unparsed_children'

        Returns -1 on error, otherwise the amount of lines in the function body
        """
        cur_ln_num = line[0].ln
        #if second last character not a colon: error
        if line[len(line)-2].token_t != "TT_colon":
            self.error = SyntaxError("Expected colon", cur_ln_num, self.file_n)
            return -1
        line = line[1:len(line)-2]
        if line[0].token_t != "TT_identifier":
            self.error = SyntaxError("Expected identifier", cur_ln_num, self.file_n)
            return -1
        func_identifier, line = line[0].token_v, line[1:]
        if line[0].token_t != "TT_lparen" or line[len(line)-1].token_t != "TT_rparen":
            self.error = SyntaxError("Expected parenthesis", cur_ln_num, self.file_n)
            return -1
        line = line[1:len(line)-1]
        args = self.parse_func_def_args(line, cur_ln_num)
        if args == -1: return args
        new_node_id = self.ast.append_node(FuncDefNode(func_identifier, args))
        self.ast.traverse_node_by_id(new_node_id)
        self.identifier_manager.set_identifier(func_identifier, self.ast.cur_node, force_global=True)
        self.ast.cur_node.identifier_container = IdentifierContainer()
        self.ast.cur_node.arg_names = args
        self.ast.cur_node.indentation = cur_line_indentation
        children_count, self.ast.cur_node.unparsed_children = self.get_children(rem_line_tokens, cur_line_indentation)
        if "--import" in self.flags.keys():
            self.parse_no_discard_func_def()
        self.ast.detraverse_node()
        return children_count
    
    def parse_no_discard_func_def(self) -> None:
        arg_types = [() for i in range(len(self.ast.cur_node.arg_names))]
        self.parse_func_def_children(arg_types, self.ast.cur_node, self.ast.cur_node)
        return None
    
    def parse_func_def_args(self, tokens: list[Token], cur_ln_num: int) -> list[str] | int:
        """
        Takes the tokens representing the names of the function arguments and the current line number

        Iterates over each token, verifing that only valid tokens are used,
        Throws a SyntaxError if unexpected token is found or if expression ends in a comma

        Returns -1 on error, otherwise a list of the argument names
        """
        if not tokens: return []
        args = []
        cur_token = None
        for token in tokens:
            cur_token = token
            if token.token_t == "TT_identifier":
                args.append(token.token_v)
                continue
            if token.token_t == "TT_comma":
                continue
            self.error = SyntaxError("Invalid Syntax", cur_ln_num, self.file_n)
            return -1
        if cur_token.token_t == "TT_comma":
            self.error = SyntaxError("Expected argument", cur_ln_num, self.file_n)
            return -1
        return args

    def parse_for_loop(self, line: list[Token], rem_line_tokens: list[list[Token]], cur_line_indentation: int) -> int:
        """
        Takes the line to be parsed, the remaining lines and the current indentation

        Verifies valid syntax,
        Creates new node inside of ast,
        Parses expression to be iterated over and validates that it is iterable,
        Creates iter variable inside of ast and gets its type using get_array_types()
        Parses the for-loop body

        Returns -1 on error, otherwise the amount of lines in the body
        """
        cur_ln_num = line[0].ln
        if line[len(line)-2].token_t != "TT_colon":
            self.error = SyntaxError("Expected colon", cur_ln_num, self.file_n)
            return -1
        line = line[1:len(line)-2]
        if line[0].token_t != "TT_identifier":
            self.error = SyntaxError("Expected identifier", cur_ln_num, self.file_n)
            return -1
        if line[1].token_t != "TT_in":
            self.error = SyntaxError("Invalid Syntax", line[0].ln, self.file_n)
            return -1
        iter_var_name = line[0].token_v
        new_node_id = self.ast.append_node(ForLoopNode(iter_var_name))
        line = line[2:]
        self.ast.traverse_node_by_id(new_node_id)
        expr_type = self.parse_expression(line, "iter", cur_ln_num, expr_2=False, expr_4=False, expr_6=False, expr_7=False, expr_8=False)
        if expr_type == -1: return -1
        if not self.is_valid_type(expr_type, ("str", "list")): 
            self.error = TypeError(f"Object of type '{expr_type} is not iterable'", cur_ln_num, self.file_n)
            return -1
        iter_var_node = AssignNode(iter_var_name)
        self.ast.append_node(iter_var_node)
        if self.ast.cur_node.iter.name in BUILT_IN_FUNC_NAMES:
            self.ast.cur_node.children[0].type = ()
        else:
            self.ast.cur_node.children[0].type = tuple(set(self.get_array_types()))
        self.identifier_manager.set_identifier(iter_var_name, iter_var_node)
        child_count = self.parse_children(cur_line_indentation, rem_line_tokens)
        self.ast.detraverse_node()
        return child_count

    def parse_while_loop(self, line: list[Token], rem_line_tokens: list[list[Token]], cur_line_indentation: int) -> int:
        """
        Takes the line to be parsed, the remaining lines and the current indentation

        Verifies valid syntax,
        Creates new node inside the ast,
        Parses the expression representing the loop condition
        Calls parse_children() on the loop body

        Returns -1 on error, otherwise the amount of lines inside the body
        """
        cur_ln_num = line[0].ln
        if line[len(line)-2].token_t != "TT_colon":
            self.error = SyntaxError("Expected colon", line[0].ln, self.file_n)
            return -1
        line = line[1:len(line)-2]
        new_node_id = self.ast.append_node(WhileLoopNode())
        self.ast.traverse_node_by_id(new_node_id)
        expr_type = self.parse_expression(line, "condition",cur_ln_num,  expr_4=False)
        if expr_type == -1: return -1
        if not self.is_valid_type(expr_type, ("bool",)):
            self.error = TypeError("Invalid type for conditional expression", cur_ln_num, self.file_n)
            return -1
        child_count = self.parse_children(cur_line_indentation, rem_line_tokens)
        self.ast.detraverse_node()
        return child_count
        
    def parse_conditional_statement(self, line: list[Token], rem_line_tokens: list[list[Token]], cur_line_indentation: int) -> int:
        """
        Takes the line to be parsed, the remaining lines and the current indentation

        Verifies valid syntax,
        Creates a new node inside the ast,
        Verifies that else- and elif statements are preceded by an if- or elif statement,
        Parses the conditional expression,
        Parses the body

        Returns -1 on error, otherwise the amount of lines in the body
        """
        cur_ln_num = line[0].ln
        if line[len(line)-2].token_t != "TT_colon":
            self.error = SyntaxError("Expected colon at end of conditional statement", cur_ln_num, self.file_n)
            return -1
        match line[0].token_t:
            case "TT_if": kw_node = IfNode()
            case "TT_elif": kw_node = ElifNode()
            case "TT_else": kw_node = ElseNode()
        new_node_id = self.ast.append_node(kw_node)
        self.ast.traverse_node_by_id(new_node_id)
        if line[0].token_t in ("TT_elif", "TT_else"):
            if self.get_prev_conditional_nodes() == -1: 
                self.error = SyntaxError("Expected conditional statement before 'else/elif'", line[0].ln, self.file_n)
                return -1
        line = line[1:len(line)-2]
        if not isinstance(self.ast.cur_node, ElseNode):
            if self.parse_expression(line, "condition", cur_ln_num, expr_4=False) == -1: return -1
        child_count = self.parse_children(cur_line_indentation, rem_line_tokens)
        self.ast.detraverse_node()
        return child_count

    def parse_children(self, parent_line_indentation: int, child_line_tokens: list[list[Token]] = None, block_to_parse: list[list[Token]] = None) -> int:
        """
        Takes the indentation of the parent line and the remaining lines or a preprocessed block of tokens to parse

        If all the remaining lines have been passed, the block to parse has to be obtained by calling get_children(),
        The block of lines is split back into tokens,
        The current indentation is kept track off and parse_block is called

        Returns -1 on error, otherwise the amount of lines in the block
        """
        if block_to_parse:
            children_count = len(block_to_parse)
        else: 
            result = self.get_children(child_line_tokens, parent_line_indentation)
            if result == -1: return -1
            children_count, block_to_parse = result
        block_to_parse = [token for line in block_to_parse for token in line]
        old_cur_block_indentation = self.cur_block_indentation
        self.cur_block_indentation = None
        if self.parse_block(block_to_parse) == -1: return -1
        self.cur_block_indentation = old_cur_block_indentation
        return children_count

    def get_prev_conditional_nodes(self) -> None:
        """
        Traverses through the previously parsed nodes in the current block
        When prev_child_node() returns -1, it has reached the first line in the block, so we end the loop
        Exit loop if the current node is not a conditional statement
        Store nodes in list, return -1 if list is empty and append nodes into ast

        Returns -1 on error, otherwise 0
        """
        cur_node = self.ast.cur_node
        prev_cond_nodes = []
        while True:
            if self.ast.prev_child_node() == -1: break
            if not isinstance(self.ast.cur_node, (IfNode, ElifNode)):
                break
            prev_cond_nodes.append(self.ast.cur_node)
        if prev_cond_nodes == []:
            return -1
        self.ast.cur_node = cur_node
        for node in prev_cond_nodes:
            self.ast.append_node(node, "prev_conditions")
        return 0

    def parse_func_call(self, line: list[Token], traversal_type: str="children", from_expr: bool = False) -> int:
        """
        Takes the current line, the name of the branch of the current node, the function call is situated at
        and the fact if the function is called from parse_expression() of from parse_line()

        Verifies that the function called exists,
        Verifies valid syntax, taking the EOL token into consideration if the function is called from parse_line(),
        Parses the arguments passed to the function by calling parse_func_call_args(),
        Ensures valid syntax for arguments passed to the function,
        Creates a new node inside the ast,
        Calls parse_expression on each argument and stores its type,
        Ensures that the amount of arguments given corresponds to the expected amount,
        Stores the new node inside the corresponding FuncDefNode,
        Parses the children of the FuncDefNode if they are still unparsed
        Verifies that the function arguments have the expected type

        Returns -1 on error, otherwise 0
        """
        cur_ln_num = line[0].ln
        name = line[0].token_v
        if not self.identifier_manager.identifier_exists(name):
            self.error = NameError(f"Call to undefined function {name}()", cur_ln_num, self.file_n) 
            return -1
        if not from_expr:
            line = line[2:len(line)-2]
        else:
            line = line[2:len(line)-1]
        arg_exprs = self.parse_func_call_args(line)
        if isinstance(arg_exprs, int):
           return self.handle_arg_parsing_error(arg_exprs, cur_ln_num)
        new_node_id = self.ast.append_node(FuncCallNode(name), traversal_type)
        self.ast.traverse_node_by_id(new_node_id, traversal_type)
        new_node = self.ast.cur_node
        arg_types = []
        for arg_expr in arg_exprs:
            arg_type = self.parse_expression(arg_expr, "args", cur_ln_num, expr_4=False)
            if arg_type == -1: return -1
            if self.is_valid_type(arg_type, ("func",)):
                self.error = TypeError("Higher Order functions are not yet supported.", cur_ln_num, self.file_n)
                return -1
            arg_types.append(arg_type)
        if not arg_exprs and name == "input":
            self.ast.append_node(StringNode(""), "args")
        elif len(arg_exprs) == 1 and name == "strip":
            self.ast.append_node(StringNode(" "), "args")
        elif not arg_exprs and name == "print":
            self.ast.append_node(StringNode(""), "args")
        elif not arg_exprs and name == "exit":
            self.ast.append_node(NumberNode(0), "args")
        last_func_def_node = self.identifier_manager.get_identifier_node(name)
        var_args = self.var_args_checking(name, arg_types, cur_ln_num)
        if self.error: return -1
        if not var_args and len(arg_exprs) != len(last_func_def_node.arg_names):
            self.error = TypeError(f"{name}() takes {len(last_func_def_node.arg_names)} arguments but {len(arg_exprs)} were given!", cur_ln_num, self.file_n)
            return -1
        if last_func_def_node.func_call_nodes:
            last_func_def_node.func_call_nodes.append(new_node)
        else:
            last_func_def_node.func_call_nodes = [new_node]
        if name not in BUILT_IN_FUNC_NAMES and last_func_def_node.unparsed_children:
            if self.parse_func_def_children(arg_types, last_func_def_node, new_node) == -1: return -1
        self.ast.detraverse_node()
        return 0
    
    def handle_arg_parsing_error(self, error_code: int, cur_ln_num: int) -> int:
        """
        Takes an error code and the number of the line that is being parsed

        Matches error codes to different kinds of errors and stores them in self.error

        Returns -1
        """
        match error_code:
            case -1: 
                    self.error = SyntaxError("'(' was never closed", cur_ln_num, self.file_n)
            case -2:
                    self.error = SyntaxError("'[' was never closed", cur_ln_num, self.file_n)
            case -3:
                    self.error = SyntaxError("Invalid Syntax", cur_ln_num, self.file_n)
        return -1

    def parse_func_def_children(self, arg_types: list[str], last_func_def_node: FuncDefNode, new_node: FuncCallNode) -> None:
        """
        Takes the types of the arguments passed to a function, the corresponding FuncDefNode and the new FuncCallNode

        Keeps Track of the current node inside the internal ast,
        Stores the types of the function arguments inside the FuncDefNode, now not allowing any other types for this function,
        Stores the previously preprocessed function body inside a variable
        Creates AssignNodes representing the function arguments inside the ast, just giving them a type but not a value
        Calls parse_children()

        Returns -1 on error, otherwise 0
        """
        self.ast.cur_node = last_func_def_node
        self.ast.cur_node.arg_types = arg_types
        block_to_parse = self.ast.cur_node.unparsed_children
        self.ast.cur_node.unparsed_children = None
        for i in range(len(arg_types)):
            self.ast.append_node(AssignNode(self.ast.cur_node.arg_names[i]))
            self.identifier_manager.set_identifier(self.ast.cur_node.arg_names[i], self.ast.cur_node.children[i])
            self.ast.cur_node.children[i].type = arg_types[i]
        if self.parse_children(self.ast.cur_node.indentation, block_to_parse=block_to_parse) == -1: return -1
        if not self.ast.cur_node.return_nodes:
            self.ast.append_node(ReturnNode(), "return_nodes")
            self.ast.traverse_node("return_nodes")
            self.ast.append_node(NoneNode(), "return_value")
            self.ast.detraverse_node()
            self.ast.cur_node.children.append(self.ast.cur_node.return_nodes[0])
        self.ast.cur_node = new_node
        return 0
    
    def parse_func_call_args(self, tokens: list[Token]) -> list[list[Token]]:
        """
        Takes the tokens representing the arguments passed to the function being called

        Iterates over each token, keeping track of parenthesis- and bracket-depth and acting accordingly, splitting the tokens into expressions with the comma as a seperator,
        
        Returns an error code between -1 and -3 on error, otherwise a list of the expression passed as arguments
        """
        if not tokens: return []
        arg_exprs = []
        cur_arg_expr = []
        paren_depth = 0
        bracket_depth = 0
        for token in tokens:
            if token.token_t in ("TT_lparen", "TT_rparen", "TT_lbracket", "TT_rbracket"):
                match token.token_t:
                    case "TT_lparen":
                        paren_depth += 1
                    case "TT_rparen":
                        if paren_depth == 0: return -1
                        paren_depth -= 1
                    case "TT_lbracket":
                        bracket_depth += 1
                    case "TT_rbracket":
                        if bracket_depth == 0: return -1
                        bracket_depth -= 1
                cur_arg_expr.append(token)
                continue
            if token.token_t == "TT_dquote" or token.token_t == "TT_squote":
                continue
            elif token.token_t == "TT_comma" and not paren_depth and not bracket_depth:
                arg_exprs.append(cur_arg_expr)
                cur_arg_expr = []
            else:
                cur_arg_expr.append(token)
        if paren_depth:
            return -1
        if bracket_depth:
            return -2
        if cur_arg_expr == []:
            return -3
        arg_exprs.append(cur_arg_expr)
        return arg_exprs
    
    def parse_assign(self, line: list[Token]) -> int:
        """
        Takes the line to be parsed

        Creates a new node inside the ast,
        Parses the expression on the right side of the assignement operator
        Calls get_array_types() if expression is a list to keep track of the types of the objects inside the list
        Sets the type attribute of the node inside the ast equal to the type returned by parse_expression()
        Logs the variable in the var identifier dict of the current scope

        Returns -1 on error, otherwise 0
        """
        cur_ln_num = line[0].ln
        line = line[:len(line)-1]
        name = line[0].token_v
        if name in ("argc", "argv"):
            self.warning = Warning(f"Overwriting built-in constant '{name}' might lead to unexpected behavior", cur_ln_num, self.file_n)
        var_type = self.parse_assignment_expression(line, "children", cur_ln_num)
        if var_type == -1:
            return -1
        if var_type != () and self.is_valid_type(var_type, ("list",)):
            if isinstance(self.ast.cur_node.value, ArrayNode):
                self.ast.cur_node.children_types = tuple(set([child.type for child in self.ast.cur_node.value.children]))
            elif (self.ast.cur_node.value.__class__.__name__ == "FuncCallNode" and self.ast.cur_node.value.name != "range"):
                self.ast.cur_node.children_types = self.get_array_types()
            self.ast.cur_node.children_count = len(self.ast.cur_node.children_types)
        self.ast.detraverse_node()
        return 0
    
    #################################Expression Parsing###############################

    def parse_expression(self, tokens: list[Token], traversal_type: str, ln_num: int, **exclude: bool) -> str | int:
        """
        Takes the tokens representing the expression, the branch of the current node where the expression is expected,
        the line number and expressions to not allow as keyword args

        Removes the EOL token if existent
        Removes outside parenthesis
        Ensures that the expression exists
        Keeps track of what expressions to look for
        Calls the corresponding handler function for each type of expression depending on different criteria
        Throws an error if tokens are not recognized as a valid expression

        Returns -1 on error, otherwise the type of the expression that has been parsed
        """
        tokens = [token for token in tokens if token.token_t != "TT_eol"]
        no_tokens = True if not tokens else False
        if not no_tokens and tokens[0].token_t == "TT_lparen" and tokens[-1].token_t == "TT_rparen":
            tokens = tokens[1:-1]
        if not tokens or no_tokens:
            self.error = SyntaxError("Invalid Expression", ln_num, self.file_n)
            return -1
        cur_expr_map = EXPR_MAP.copy()
        for kw in exclude.keys():
            kw_int = int(kw[-1])
            if kw_int not in EXPR_MAP:
                raise Exception("Invalid Argument for function 'parse_expression': Argument must be in scope of EXPR_MAP!")
            cur_expr_map[kw_int] = False
        if cur_expr_map[0] and len(tokens) == 1:
            return self.parse_simple_literal_and_var(tokens[0], traversal_type)
        if cur_expr_map[1] and self.is_array_literal(tokens):
            return self.parse_array_literal(tokens, traversal_type)
        if len(tokens) >= 3 and cur_expr_map[3] and self.is_array_var(tokens):
            return self.parse_array_var(tokens, traversal_type)
        if len(tokens) >= 3 and cur_expr_map[5] and tokens[0].token_t == "TT_identifier" and tokens[1].token_t == "TT_lparen" and tokens[len(tokens)-1].token_t == "TT_rparen" and self.is_func_call(tokens):
            if self.parse_func_call(tokens, traversal_type, from_expr=True) == -1: return -1
            return self.func_identifier_dict[tokens[0].token_v].return_type
        if cur_expr_map[6] and tokens[0].token_t in ("TT_sub", "TT_not", "TT_plus"):
            return self.parse_un_op_expression(tokens, traversal_type, ln_num)
        for i, token in enumerate(tokens):
            if i == 0 or i == len(tokens)-1:
                continue
            if cur_expr_map[8] and token.token_t in ("TT_equ", "TT_less", "TT_greater", "TT_and", "TT_or", "TT_dequ", "TT_gequ", "TT_lequ") or (token.token_t in ("TT_equ", "TT_less", "TT_greater") and tokens[i+1] == "TT_equ"):
                return self.parse_conditional_expression(tokens, traversal_type)
            if cur_expr_map[7] and token.token_t in ("TT_sub", "TT_plus", "TT_mul", "TT_div", "TT_mod") and not [token for token in tokens if token.token_t in ("TT_equ", "TT_greater", "TT_less")]:
                return self.parse_binop_expression(tokens, traversal_type)
        if cur_expr_map[4] and [token for token in tokens if token.token_t == "TT_colon"]:
            return self.parse_slice_expression(tokens, traversal_type, ln_num)
        self.error = SyntaxError("Invalid Expression", ln_num, self.file_n)
        return -1

    def parse_assignment_expression(self, tokens: list[Token], traversal_type: str, ln_num: int) -> str | int:
        tokens = self.merge_equ(tokens)
        name = tokens[0].token_v
        operator_idx = self.get_operator_info(tokens, ASSIGNMENT_OPERATOR_PRECEDENCE_DICT)[1]
        left = tokens[:operator_idx]
        right = tokens[operator_idx+1:]
        new_node_id = self.ast.append_node(AssignNode(name))
        self.ast.traverse_node_by_id(new_node_id)
        if self.identifier_manager.identifier_exists(left[0].token_v):
            old_def = self.identifier_manager.get_identifier_node(left[0].token_v)
            self.ast.cur_node.first_define = False
        else:
            self.ast.cur_node.first_define = True
            self.identifier_manager.set_identifier(left[0].token_v, self.ast.cur_node)
        expr_type = self.parse_expression(right, "value", ln_num, expr_4=False)
        self.ast.cur_node.type = expr_type
        if expr_type == -1: return -1
        if self.parse_expression(left, "left_expr", ln_num, expr_1=False, expr_2=False, expr_4=False, expr_5=False, expr_6=False, expr_7=False, expr_8=False) == -1:
            self.error = SyntaxError("Invalid left-side indexing expression", ln_num, self.file_n)
            return -1
        if self.ast.cur_node.left_expr.__class__.__name__ == "ArrayVarNode":
            if self.ast.cur_node.first_define:
                self.error = SyntaxError("Invalid Syntax", ln_num, self.file_n)
                return -1
            if not self.is_valid_type(old_def.type, ("list", "str")):
                self.error = TypeError("Cannot index non-container type", ln_num, self.file_n)
                return -1
        self.identifier_manager.set_identifier(left[0].token_v, self.ast.cur_node)

        return expr_type

    def parse_slice_expression(self, tokens: list[Token], traversal_type: str, ln_num: int) -> str | int:
        """
        Takes the tokens representing the expression to be parsed, the branch of the current node where the expression is found
        and the current line number

        Verifies valid syntax
        Calls get_operator_info to obtain the leftmost valid operator and its index
        Creates a new node inside the ast
        Handles special cases such as e.g. x[1:] or x[:1]
        Calls parse sides if expression has two sides
        Verifies that the expression is of type int

        Returns -1 on error, otherwise ("int",) as slice expressions can only be of type int
        """
        if len(tokens) == 1:
            self.error = SyntaxError("Invalid syntax", tokens[0].token_t, self.file_n)
            return -1
        operator, operator_idx = self.get_operator_info(tokens, SLICE_OPERATOR_PRECEDENCE_DICT, is_slice_expr = True)
        if operator == -1: return -1
        new_node_id = self.ast.append_node(SliceExpressionNode(), traversal_type)
        self.ast.traverse_node_by_id(new_node_id, traversal_type)
        if operator_idx == len(tokens)-1 or operator_idx == 0:
            if operator_idx == 0:
                tokens_to_parse = tokens[1:]
                traversal_type = "right"
            else:
                tokens_to_parse = tokens[:len(tokens)-1]
                traversal_type = "left"
            expr_type = self.parse_expression(tokens_to_parse, traversal_type, ln_num)
        else:
            left = tokens[:operator_idx]
            right = tokens[operator_idx+1:]
            expr_type = self.parse_sides(left, right, ln_num, SLICE_EXPRESSION_ALLOWED_TYPES, ":")
        if expr_type == -1: return -1
        if not self.is_valid_type(expr_type, SLICE_EXPRESSION_ALLOWED_TYPES):
            self.error = TypeError(f"Invalid type for slice expression: '{expr_type}'", tokens[0].token_t, self.file_n)
            return -1
        self.ast.detraverse_node()
        return expr_type

    def parse_conditional_expression(self, tokens: list[Token], traversal_type: str) -> str | int:
        """
        Takes the tokens representing the expression to be parsed and the branch of the current node where the expression is found

        Checks if the operator tokens in tokens have been preprocessed and preprocesses them if they haven't
        Obtains the leftmost valid operator with the highest precedence and its index
        Splits the tokens into a left and a right side
        Creates a new node inside the ast
        Handles different allowed types depending on the operator
        Calls parse_sides()
        Stores the type of the expression in the type attribute of the new node

        Returns -1 on error, otherwise the type of the expression
        """
        if not [token for token in tokens if token.token_t in ("TT_dequ", "TT_gequ", "TT_jequ")]:
            tokens = self.merge_equ(tokens)
        operator, operator_idx = self.get_operator_info(tokens, CONDITION_OPERATOR_PRECEDENCE_DICT)
        if operator == -1: return -1
        operator_str = get_token_ident(operator.token_t)
        left = tokens[:operator_idx]
        right = tokens[operator_idx+1:]
        new_node_id = self.ast.append_node(ConditionExpressionNode(operator_str), traversal_type)
        self.ast.traverse_node_by_id(new_node_id, traversal_type)
        if operator.token_t in ("TT_and", "TT_or"):
            allowed_types = CONDITIONAL_EXPRESSION_LOGICAL_OPERATOR_ALLOWED_TYPES
        else:
            allowed_types = CONDITIONAL_EXPRESSION_CONDTIONAL_OPERATOR_AllOWED_TYPES
            if operator_str in ["==", "!="]:
                allowed_types += CONDITIONAL_EXPRESSION_DEQU_ALLOWED_TYPES
        if self.parse_sides(left, right, tokens[0].ln, allowed_types, operator_str) == -1: return -1
        self.ast.cur_node.type = ("bool",)
        self.ast.detraverse_node()
        return ("bool",)
        
    def parse_binop_expression(self, tokens: list[Token], traversal_type: str) -> str | int:
        """
        Takes the tokens representing the expression to be parsed and the branch of the current node where the expression is found

        Obtains the leftmost highest precedence operator and its index
        Handles different allowed types based on the operator
        Splits the tokens into left and right
        Creates a new node inside the ast
        Calls parse_sides()
        Stores the expression type in the type attribute of the new node

        Returns -1 on error, otherwise the type of the expression
        """
        operator, operator_index = self.get_operator_info(tokens, BINOP_OPERATOR_PRECEDENCE_DICT)
        if operator == -1: return -1
        allowed_types = BINOP_EXPRESSION_BASE_ALLOWED_TYPES
        if operator.token_t == "TT_plus":
            allowed_types += BINOP_EXPRESSION_PLUS_ALLOWED_TYPES
        if operator.token_t == "TT_mul":
            allowed_types += BINOP_EXPRESSION_MUL_ALLOWED_TYPES
        if operator.token_t ==  "TT_mod":
            allowed_types = BINOP_EXPRESSION_MOD_ALLOWED_TYPES
        left = tokens[:operator_index]
        right = tokens[operator_index+1:]
        new_node_id = self.ast.append_node(BinOpNode(get_token_ident(operator.token_t)), traversal_type)
        self.ast.traverse_node_by_id(new_node_id, traversal_type)
        expr_type = self.parse_sides(left, right, tokens[0].ln, allowed_types, get_token_ident(operator.token_t))
        if expr_type == -1: return -1    
        self.ast.cur_node.type = expr_type
        self.ast.detraverse_node()
        return expr_type
    
    def get_operator_info(self, tokens: list[Token], operator_precedence_dict: dict[str: int], is_slice_expr: bool = False) -> tuple[str | int, int]:
        """
        Take the tokens representing an expression, the corresponding operator precedence dict and the fact if the expression is a slice expression

        Iterates over each token
        Keeps track of bracket and parenthesis depth
        Verifies that the operator is in a valid spot
        Verifies that all brackets and parenthesis have been closed and that an operator exists

        Returns (-1, -1) on error, otherwise the found operator and its index
        """
        operator_precedence = operator_precedence_dict
        best_operator = None
        paren_depth = 0
        bracket_depth = 0
        for i, token in enumerate(tokens):
            if token.token_t in ("TT_lparen", "TT_lbracket", "TT_rparen", "TT_rbracket"):
                match token.token_t:
                    case "TT_lparen":
                        paren_depth += 1
                    case "TT_lbracket":
                        bracket_depth += 1
                    case "TT_rparen":
                        paren_depth -= 1
                    case "TT_rbracket":
                        bracket_depth -= 1
                continue
            if paren_depth or bracket_depth or token.token_t not in operator_precedence.keys():
                continue
            if i == 0 or i == len(tokens)-1 and not is_slice_expr:
                self.error = SyntaxError("Invalid Syntax", tokens[0].ln, self.file_n)
                return -1, -1
            if operator_precedence[token.token_t] == 2:
                if not best_operator:
                    best_operator = token
                    continue
            best_operator = token
            break
        if not best_operator or paren_depth or bracket_depth:
            self.error = SyntaxError("Invalid Syntax", tokens[0].ln, self.file_n)
            return -1, -1
        return best_operator, tokens.index(best_operator)
    
    def parse_sides(self, left: list[Token], right: list[Token], ln_num: int, allowed_types: list[tuple[str, str]], operator: str) -> str | int:
        """
        Takes the left and right sides of an expression as tokens, the line number, the allowed types for the expression and the operator of the expression

        Calls parse_expression() on each side,
        Calls resolve_types() to combine the types of the sides into one type
        Ensures that the expression type is valid with a call to is_valid_type()

        Returns -1 on error, otherwise the type of the expression
        """
        left_expr_type = self.parse_expression(left, "left", ln_num, expr_4=False)
        if left_expr_type == -1: return -1
        right_expr_type = self.parse_expression(right, "right", ln_num, expr_4=False)
        if right_expr_type == -1: return -1
        expr_type = self.resolve_types(left_expr_type, right_expr_type, operator, ln_num)
        if expr_type == -1: return -1
        if not self.is_valid_type(expr_type, allowed_types):
            self.error = TypeError("Invalid Expression type", ln_num, self.file_n)
            return -1
        return expr_type

    def parse_un_op_expression(self, tokens: list[Token], traversal_type: str, ln_num) -> str | int:
        """
        Takes the tokens representing the expression and the branch of the current node where the expression is situated

        Obtains the operator
        Creates a new node inside the ast
        Calls parse_expression() on the tokens
        Ensures a valid expression type with calls to is_valid_expression() dependent on the operator

        Returns -1 on error, otherwise the type of the expression
        """
        operator = UNOP_OPERATOR_DICT[tokens[0].token_t]
        if operator == "+":
            return self.parse_expression(tokens[1:], traversal_type, ln_num)
        node_id = self.ast.append_node(UnOpNode(operator), traversal_type)
        self.ast.traverse_node_by_id(node_id, traversal_type)
        expr_type = self.parse_expression(tokens[1:], "right", tokens[0].ln, expr_1=False, expr_3=False, expr_4=False, expr_6=False)
        if operator == "-" and not self.is_valid_type(expr_type, UNOP_EXPRESSION_SUB_ALLOWED_TYPES):
            self.error = TypeError(f"Bad type for unary operator '-': {expr_type}", tokens[0].ln, self.file_n)
            return -1
        if operator == "not" and not self.is_valid_type(expr_type, UNOP_EXPRESSION_NOT_ALLOWED_TYPES):
            self.error = TypeError(f"Bad type for unary operator 'not': {expr_type}", tokens[0].ln, self.file_n)
            return -1
        self.ast.detraverse_node()
        return expr_type
        
    def parse_array_var(self, tokens: list[Token], traversal_type: str) -> str | int:
        """
        Takes the tokens representing the expression and the branch of the current node where the expression is situated

        Verifies that the variable exists
        Verifies that the variable has a subscriptable type
        Verifies valid syntax
        Creates a new node inside the ast
        Calls parse_expression() on the indexing expression
        Ensures that the indexing expression is of type "int"

        Returns -1 on error, otherwise () (feature not yet supported, still looking for a solution to some problems)
        """
        cur_ln_num = tokens[0].ln
        var_identifier = tokens[0].token_v
        if not self.identifier_manager.identifier_exists(var_identifier):
            self.error = NameError(f"Name {var_identifier} is not defined", cur_ln_num, self.file_n)
            return -1
        var_dec_node = self.identifier_manager.get_identifier_node(var_identifier)
        self.ast.cur_node.child_count = var_dec_node.child_count
        if not self.is_valid_type(var_dec_node.type, ("str", "list")):
            self.error = TypeError(f"{var_dec_node.type} object is not subscriptable", cur_ln_num, self.file_n)
            return -1
        content_exprs = self.get_content_expressions(tokens[1:])
        if content_exprs == -1: return -1
        tokens = tokens[2:len(tokens)-1]
        new_node_id = self.ast.append_node(ArrayVarNode(var_identifier), traversal_type)
        self.ast.traverse_node_by_id(new_node_id, traversal_type)
        for content_expr in content_exprs:
            expr_type = self.parse_expression(content_expr, "content", cur_ln_num, expr_1=False, expr_3=False, expr_8=False)
            if expr_type == -1: return -1
            if not self.is_valid_type(expr_type, ("int",)):
                self.error = TypeError(f"list indices must be of type int, not {expr_type}", cur_ln_num, self.file_n)
                return -1 
        self.ast.detraverse_node()
        return () #Return empty tuple to mark variable as any type as empty tuple will pass all is_valid_type() checks
    
    def get_content_expressions(self, tokens: list[Token]) -> list[list[Token]] | int:
        content_expressions = []
        cur_content_expr = []
        bracket_depth = 0
        for token in tokens:
            if token.token_t == "TT_lbracket": 
                bracket_depth += 1
                continue
            if token.token_t == "TT_rbracket":
                if bracket_depth == 0:
                    self.error = SyntaxError("']' was never opened", token.ln, self.file_n)
                    return -1
                if bracket_depth == 1:
                    content_expressions.append(cur_content_expr)
                    cur_content_expr = []
                bracket_depth -= 1
                continue
            if bracket_depth == 0 and token.token_t != "TT_lbracket":
                self.error = SyntaxError("Unexpected Token", token.ln, self.file_n)
                return -1
            cur_content_expr.append(token)
        if bracket_depth:
            self.error = SyntaxError("'[' was never closed", tokens[0].ln, self.file_n)
            return -1

        return content_expressions

        
    def parse_array_literal(self, tokens: list[Token], traversal_type: str) -> str | int:
        """
        Takes the tokens representing the expression and the branch of the current node where the expression is situated

        Handles the case of an empty list
        Extracts the different expressions representing the list elements
        Ensures valid syntax, e.g. not this: x = [1, 2,]
        Creates a new node inside the ast
        Calls parse_expression on each expression representing an element

        Returns -1 on error, otherwise ("list",)
        """
        tokens = tokens[1:len(tokens)-1]
        if tokens == []:
            self.ast.append_node(ArrayNode(), traversal_type)
            return ("list",)
        arr_element_expressions = []
        cur_arr_element_expression = []
        paren_depth = 0
        for token in tokens:
            if token.token_t in ("TT_lparen", "TT_lbracket"):
                paren_depth += 1
            if token.token_t in ("TT_rparen", "TT_rbracket"):
                if paren_depth == 0:
                    self.error = SyntaxError("Invalid Syntax", tokens[0].token_t, self.file_n)
                    return -1
                paren_depth -= 1
            if token.token_t == "TT_comma" and not paren_depth:
                if cur_arr_element_expression:
                    arr_element_expressions.append(cur_arr_element_expression)
                    cur_arr_element_expression = []
                    continue
                else:
                    self.error = SyntaxError("Invalid Syntax", token.ln, self.file_n)
                    return -1
            cur_arr_element_expression.append(token)
        if not cur_arr_element_expression:
            self.error = SyntaxError("Invalid Syntax", token.ln, self.file_n)
            return -1
        arr_element_expressions.append(cur_arr_element_expression)
        node_id = self.ast.append_node(ArrayNode(), traversal_type)
        self.ast.traverse_node_by_id(node_id, traversal_type)
        for arr_element_expression in arr_element_expressions:
            if self.parse_expression(arr_element_expression, "children", tokens[0].ln, expr_4=False) == -1: return -1
        self.ast.detraverse_node()
        return ("list",)
        
    def parse_simple_literal_and_var(self, token: Token, traversal_type: str) -> str | int:
        """
        Takes the token representing the simple literal or var and the branch of the current node where the expression is situated

        Checks for differently typed literals, creates a new node inside the ast
        In case of a variable:
            Checks if the variable exists in the current scope
            Creates a new node inside the ast

        Returns -1 on error, otherwise the type of the variable or literal
        """
        if token.token_t in ("TT_int", "TT_float"):
            new_node_id = self.ast.append_node(NumberNode(token.token_v), traversal_type)
            self.ast.traverse_node_by_id(new_node_id, traversal_type)
            self.ast.cur_node.type = token.token_t[3:]
            self.ast.detraverse_node()
            return (token.token_t[3:],)
        if token.token_t == "TT_bool":
            self.ast.append_node(BoolNode(token.token_v), traversal_type)
            return ("bool",)
        if token.token_t == "TT_str":
            self.ast.append_node(StringNode(token.token_v), traversal_type)
            return ("str",)
        if token.token_t == "TT_none":
            self.ast.append_node(NoneNode(), traversal_type)
            return ("none",)
        if token.token_t == "TT_identifier":
            name = token.token_v
            if not self.identifier_manager.identifier_exists(name):
                self.error = NameError(f"Unknown Identifier: '{name}'", token.ln, self.file_n)
                return -1
            new_node_id = self.ast.append_node(VarNode(name), traversal_type)
            self.ast.traverse_node_by_id(new_node_id, traversal_type)
            cur_node_type = self.identifier_manager.get_identifier_node(name).type
            self.ast.cur_node.type = cur_node_type
            self.ast.detraverse_node()
            return cur_node_type
            
        self.error = SyntaxError("Invalid Literal", token.ln, self.file_n)
        return -1
    
    ########################################Misc########################################
    
    def resolve_types(self, left_expr_type: tuple[str], right_expr_type: tuple[str], operator: str, ln_num: int) -> tuple[str]:
        """
        Takes the type of the left expression, the type of the right expression, the operator of the bigger expression and the line number

        Handles different combinations of types and operators:
            Returns ("bool",) if the operator is == or !=
            Returns the type of the left expression if both sides have the same type and takes into account that the left type might be a union type
            Returns ("str",) if the sides have the type "str" and "int" and the operator is "*"
            Returns ("float",) if the sides have the type "int" and "float"
            All other combinations of types except union types are invalid and lead to an error
        If one or more sides are of the union type, resolve_types() is called on each possible combination of types generated by get_combinations()
        Duplicates are eliminated by converting the list to a set and then back to a list
        Then this list is returned as a tuple

        Returns -1 on error
        """
        if left_expr_type == None or right_expr_type == None or () in left_expr_type or () in right_expr_type:
            return ()
        types = []
        if operator in ("==", "!="):
            return ("bool",)
        if left_expr_type == right_expr_type:
            return left_expr_type if isinstance(left_expr_type, tuple) else (left_expr_type,)
        if len(left_expr_type) == len(right_expr_type) and len(left_expr_type) == 1:
            expr_type = (left_expr_type[0], right_expr_type[0])
            if self.is_valid_type(expr_type, ("int", "str")):
                if operator == "*": return ("str",)
                self.error = TypeError("Invalid combination of types 'int', 'str'", ln_num, self.file_n)
                return -1
            if self.is_valid_type(expr_type, ("float", "int")): 
                return ("float",)
            self.error = TypeError(f"Invalid combination of types: '{expr_type[0]}', '{expr_type[1]}'", ln_num, self.file_n)
            return -1
        combinations = get_combinations(left_expr_type, right_expr_type)
        for combination in combinations:
            combination_type = self.resolve_types((combination[0],), (combination[1],), operator, ln_num)
            if combination_type == -1: return -1
            types += list(combination_type)
        return tuple(set(types))

    def is_valid_type(self, t1: tuple[str], t2: tuple[str]) -> bool:
        """
        Takes two tuples containing a normal or union type

        Ensures that the possible union type to be compared does not contain more types than the union type to be compared against
        Sorts both tuples
        
        Returns True if the first tuple is a sublist of the second, otherwise False
        """
        if t1 == None:
            return True
        if len(t1) > len(t2):
            return False
        if len(t1) > 1:
            t1 = tuple(sorted(t1))
        if len(t2) > 1:
            t2 = tuple(sorted(t2))
        if t1 not in get_sublists(t2):
            return False
        return True

    def is_array_literal(self, tokens: list[Token]) -> bool:
        """
        Takes the tokens to check and the fact if we are checking for an array literal or an indexing expression

        Returns False if we are looking for a literal and the expression is not wrapped in brackets
        Iterates over each token
        Checks if a bracket has been opened and then closed again, making the expression contain multiple lists and making it a BinOpExpression

        Returns True if no bracket has been closed on the lowest bracket depth, otherwise False
        """
        if tokens[0].token_t != "TT_lbracket" or tokens[-1].token_t != "TT_rbracket":
            return False
        bracket_depth = 0
        for i, token in enumerate(tokens):
            match token.token_t:
                case "TT_lbracket":
                    bracket_depth += 1
                case "TT_rbracket":
                    if bracket_depth != 1 or i == len(tokens)-1:
                        bracket_depth -= 1
                    else:
                        return False
        return True

    def is_array_var(self, tokens: list[Token]) -> bool:
        if tokens[0].token_t != "TT_identifier": return False
        tokens = tokens[1:]
        bracket_depth = 0
        for token in tokens:
            if token.token_t == "TT_lbracket":
                bracket_depth += 1
            if token.token_t == "TT_rbracket":
                if bracket_depth == 0: return False
            if bracket_depth == 0 and token.token_t != "TT_lbracket": return False

        return True
    
    def get_array_types(self) -> list[str]:
        """
        Keeps track of the original current node inside the ast
        Calls get_iter_nodes() to obtain all ArrayNodes that make up the Varnode from which the function has been called, 
        even if multiple ArrayNode have been joined together with BinopNodes or other operations

        Checks if get_iter_nodes has found an AssignNode that directly stores the type of its children, making it return the types
        Iterates over each ArrayNode and stores the types of their children

        Returns -1 on error, otherwise the types of the children of the ArrayNodes making up the current node inside the ast
        """
        cur_node = self.ast.cur_node
        array_nodes = self.get_iter_nodes()
        self.ast.cur_node = cur_node
        if array_nodes == -1: return -1
        if isinstance(array_nodes, tuple):
            return array_nodes
        types = []
        for array_node in array_nodes:
            if array_node.children:
                for child in array_node.children:
                    types.append(child.type)
        return tuple(types), 
    
    def get_iter_nodes(self) -> list[ASTNode] | tuple[str]:
        """
        Checks for different current nodes inside the ast and acts accordingly:
        Returns an ArrayNode if it finds one
        Returns the children_types of an AssignNode if it has ones
        Otherwise recursively traverses down the ast

        Returns the ArrayNodes found during the traversal or the direct children types found in an AssignNode
        """
        if isinstance(self.ast.cur_node, ForLoopNode):
            if isinstance(self.ast.cur_node.iter, ArrayNode):
                return [self.ast.cur_node.iter]
            self.ast.traverse_node("iter")
            return self.get_iter_nodes()
        if isinstance(self.ast.cur_node, ArrayNode):
            return [self.ast.cur_node]
        if isinstance(self.ast.cur_node, AssignNode):
            if not self.ast.cur_node.value:
                parent_func_def_node = self.ast.get_parent_node(FuncDefNode)
                if not parent_func_def_node.func_call_nodes:
                    return []
                arg_index = parent_func_def_node.arg_names.index(self.ast.cur_node.name)
                self.ast.cur_node = parent_func_def_node.func_call_nodes[0].args[arg_index]
                return self.get_iter_nodes()
            if self.ast.cur_node.children_types:
                return self.ast.cur_node.children_types
            else:
                self.ast.traverse_node("value")
                return self.get_iter_nodes()
        if isinstance(self.ast.cur_node, VarNode):
            self.ast.cur_node = self.identifier_manager.get_identifier_node(self.ast.cur_node.name)
            return self.get_iter_nodes()
        if isinstance(self.ast.cur_node, BinOpNode):
            nodes = []
            cur_node = self.ast.cur_node
            self.ast.traverse_node("left")
            right_types = self.get_iter_nodes()
            if right_types == -1: return -1
            nodes += right_types
            self.ast.cur_node = cur_node
            self.ast.traverse_node("right")
            left_types = self.get_iter_nodes()
            if left_types == -1: return -1
            nodes += left_types
            self.ast.cur_node = cur_node
            return nodes
        if isinstance(self.ast.cur_node, FuncCallNode):
            if self.ast.cur_node.name == "range": return ()
            self.ast.cur_node = self.identifier_manager.get_identifier_node(self.ast.cur_node.name)
            nodes = []
            cur_node = self.ast.cur_node
            for node in self.ast.cur_node.return_nodes:
                self.ast.cur_node = node
                self.ast.traverse_node("return_value")
                nodes += self.get_iter_nodes()
            self.ast.cur_node = cur_node
            return nodes
        return []

    def merge_equ(self, tokens: list[Token]) -> list[Token]:
        """
        Takes a list of tokens representing a conditional expression

        Iterates over each token,
        Simply appends stores the token in a new list if it is not an operator or it is not followed by an equals operator
        Handles different combinations of operators with the equals operator by merging them into a single token

        Returns the new list of tokens with the merged operators
        """
        tokens_merged_equ = []
        i = 0
        while i < len(tokens):
            token = tokens[i]
            if i == len(tokens)-1:
                tokens_merged_equ.append(token)
                break
            next_token = tokens[i+1]
            if token.token_t not in ("TT_equ", "TT_greater", "TT_less", "TT_exclam"):
                tokens_merged_equ.append(token)
                i += 1
                continue
            if next_token.token_t != "TT_equ":
                tokens_merged_equ.append(token)
                i += 1
                continue
            if token.token_t == "TT_equ" and next_token.token_t == "TT_equ":
                token.token_t = "TT_dequ"
            elif token.token_t == "TT_greater" and next_token.token_t == "TT_equ":
                token.token_t = "TT_gequ"
            elif token.token_t == "TT_less" and next_token.token_t == "TT_equ":
                token.token_t = "TT_lequ"
            elif token.token_t == "TT_exclam":
                token.token_t = "TT_nequ"
            tokens_merged_equ.append(token)
            i += 2
        return tokens_merged_equ
    
    def get_children(self, child_line_tokens: list[Token], parent_line_indentation: int) -> int:
        """
        Takes the remaining lines of tokens and the indentation of the statement whose body we are trying to get

        Ensures that a block follows after the statement
        Iterates over each line following the statement
        Breaks out of the loop if indentation ends
        Ensures that consistent indentation is used

        Returns -1 on error, otherwise the amount of lines in the statement body and the lines of tokens representing the body
        """
        if not child_line_tokens:
            self.error = SyntaxError("Expected Block after statement", self.tokens[-1].ln-1, self.file_n)
            return -1
        cur_indentation = None
        block_to_parse = []
        for line in child_line_tokens:
            if line[0].token_t != "TT_pind":
                break
            pind_token = line[0]
            if not self.indentation:
                self.indentation = pind_token.token_v
            if not cur_indentation:
                if pind_token.token_v == parent_line_indentation + self.indentation:
                    cur_indentation = pind_token.token_v
                else:
                    self.error = IndentationError("Indent does not match previous Indents", pind_token.ln, self.file_n)
                    return -1
            if pind_token.token_v < cur_indentation:
                if pind_token.token_v % self.indentation == 0:
                    break
                self.error = IndentationError("Unindent does not match previous Indent", line[0].ln, self.file_n)
            block_to_parse.append(line)
        if block_to_parse == []:
            self.error = IndentationError("Expected Indent", line[0].ln, self.file_n)
            return -1
        children_count = len(block_to_parse)
        return children_count, block_to_parse
    
    def get_indentation(self, pind_token: Token) -> int:
        """
        Takes a pind (positive indent)token

        Returns 0 ih the given token is not a pind token
        Sets the current block indentation if neccessary
        Ensures that the indentation is consistent within the given block

        Returns -1 on error, otherwise the current indentation in spaces
        """
        if pind_token.token_t != "TT_pind":
            return 0
        if not self.cur_block_indentation:
            self.cur_block_indentation = pind_token.token_v
        if pind_token.token_v != self.cur_block_indentation:
            self.error = IndentationError("Unexpected Indent", pind_token.ln, self.file_n)
            return -1
        return pind_token.token_v
    
    def get_line_tokens(self, tokens: list[Token]) -> list[list[Token]]:
        """
        Takes a list of tokens

        Handles the case of an empty list of tokens being passed
        Iterates over the tokens and groups them into lines, removing quote tokens and EOL tokens

        Returns the newly grouped tokens
        """
        if not tokens: return [tokens]
        line_tokens = []
        highest_line_number = tokens[len(tokens)-1].ln
        for i in range(1, highest_line_number+1):
            line = [token for token in tokens if token.ln == i and token.token_t not in ("TT_squote", "TT_dquote")]
            if not line or len(line) < 2 or (line[0].token_t == "TT_pind" and line[1].token_t == "TT_eol"): continue
            line_tokens.append(line)
        return line_tokens
    
    def var_args_checking(self, name: str, arg_types: tuple, cur_ln_num: int) -> bool:
        if name not in VAR_ARG_BUILT_IN_FUNCS:
            return False
        if name == "print":
            for i in range(len(self.ast.cur_node.args)):
                if self.is_valid_type(self.ast.cur_node.args[i].type, ('func',)):
                    self.ast.cur_node.args[i] = StringNode(f"<function object '{self.ast.cur_node.args[i].name}'>")
        if name == "strip":
            if len(arg_types) == 1:
                if self.is_valid_type(arg_types[0], ("str",)): return True
                else:
                    self.error = TypeError("Invalid argument type for function strip()", cur_ln_num, self.file_n)
                    return False
            if len(arg_types) == 2:
                if self.is_valid_type(arg_types[0], ("str",) and self.is_valid_type(arg_types[1], ("str",))): return True
                else:
                    self.error = TypeError("Invalid argument type for function strip()", cur_ln_num, self.file_n)
                    return False
            return False
        if name == "range":
            if len(arg_types) < 1 or len(arg_types) > 3:
                return False
            for arg_type in arg_types:
                if not self.is_valid_type(arg_type, ("int",)):
                    self.error = TypeError("Invalid argument type for function range()", cur_ln_num, self.file_n)
                    return False
            return True
        if name == "input":
            if len(arg_types) > 1:
                return False
        if name == "exit":
            if len(arg_types) > 1:
                return False
            if arg_types and not self.is_valid_type(arg_types[0], ("int",)):
                return False
        return True
    
    def is_func_call(self, tokens: list[Token]) -> bool:
        paren_depth = 0
        for token in tokens:
            match token.token_t:
                case "TT_lparen": paren_depth += 1
                case "TT_rparen":
                    if paren_depth > 0:
                        paren_depth -= 1
                    else:
                        return False
            if not paren_depth and token.token_t in OPERATORS:
                return False

        return True
    
    def is_assign(self, line: list[Token]) -> bool:
        line_copy = deepcopy(line)
        if line_copy[0].token_t == "TT_identifier" and line_copy[1].token_t == "TT_equ":
            return True
        line_copy = self.merge_equ(line_copy)
        if not [token for token in line_copy if token.token_t == "TT_equ"]:
            return False
        operator_idx = self.get_operator_info(line_copy, ASSIGNMENT_OPERATOR_PRECEDENCE_DICT)[1]
        left = line_copy[:operator_idx]
        return self.is_array_var(left)